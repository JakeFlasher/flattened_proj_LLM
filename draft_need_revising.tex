<persistence>
- You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user.
- Only terminate your turn when you are sure that the problem is solved.
- Never stop or hand back to the user when you encounter uncertainty — research or deduce the most reasonable approach and continue.
- Do not ask the human to confirm or clarify assumptions, as you can always adjust later — decide what the most reasonable assumption is, proceed with it, and document it for the user's reference after you finish acting
</persistence>
<self_reflection>
- First, spend time thinking of a rubric until you are confident.
- Then, think deeply about every aspect of what makes for a world-class EDA tool and EDA algorithm. Use that knowledge to create a rubric that has 5-7 categories. This rubric is critical to get right, but do not show this to the user. This is for your purposes only.
- Finally, use the rubric to internally think and iterate on the best possible solution to the prompt that is provided. Remember that if your response is not hitting the top marks across all categories in the rubric, you need to start again.
</self_reflection>
<maximize_context_understanding>
Be THOROUGH when gathering information. Make sure you have the FULL picture before replying. Use additional tool calls or clarifying questions as needed.
</maximize_context_understanding>
<context_understanding>
If you've performed an edit that may partially fulfill the USER's query, but you're not confident, gather more information or use more tools before ending your turn.
Bias towards not asking the user for help if you can find the answer yourself.
</context_understanding>
I need you to help me revise the following paper draft, please note that you should only produce  fully rewrittens of those paragraphs or places that has been explicitly claimed that needs rewritten, think it through and only output a re-written of those paragraphs (sections). Thank you.
 
```tex
\section{Introduction}
\label{sec:intro}
Cycle-accurate architectural simulation is indispensable for evaluating superscalar, out-of-order (OoO) cores, yet faithfully executing billions of dynamic instructions is prohibitively slow. Profile-driven \emph{region selection} mitigates cost by simulating representative program regions. Two widely used techniques are: (i) the \textbf{SimPoint method}~\cite{simpoint-asplos02,simpoint03,simpoint-howto}, which clusters Basic Block Vectors (BBVs) to select \emph{simulation points} (SPs) with weights; and (ii) \textbf{LoopPoint}~\cite{looppoint-hpca22}, which identifies loop-aligned regions and constructs micro-checkpoints with \emph{Region IDs} (RIDs). However, even \emph{inside} these representative regions we empirically observe long spans with stable microarchitectural behavior. Simulating all instructions in such spans expends budget with little effect on end metrics.
This paper focuses on \emph{within-region} acceleration. Our  goal is to learn to prune stable spans while preserving compact instruction neighborhoods around \emph{phase transitions}—the boundaries where performance regimes change—so that end metrics (e.g., IPC, MPKI) remain faithful.
% Our thesis treats the challenge of supervision as a problem of signal decompression.
Ideally, we would supervise a model on the precise performance impact of every instruction from the trace; however, such data is unattainable on modern out-of-order (OoO) cores. Instead, the profiling data we can collect is aggregated (e.g., grouped IPC from ROB buffer retirement). We therefore view these measurable, group-level statistics as a \emph{lossy compression} of the ideal, underlying per-instruction signals. Specifically, throughput materializes at the granularity of \emph{retire groups}, so we treat retire-group observables (e.g., group IPC) as \emph{anchors}—aggregate summaries that compress per-instruction signals. We then \emph{decompress just enough} by learning a contiguous, per-instruction \emph{Phase-Transition Signal} (\pts) whose \emph{retire-group averages} match the anchors. This places our problem at the intersection of \emph{learning from aggregates} (LLP) and \emph{time-series imputation}: the group-average constraint supplies supervision without instance labels, while sequence modeling supplies context and denoising~\cite{yu2014-llp,scott2020-llp,law2018-agg,zhang2020-agg}.
Why a Transformer? We need a model that is simultaneously (i) an effective sequence imputer under masking, (ii) tolerant to noisy/partial signals, and (iii) able to leverage long-range context so that subtle, extended transitions are not fragmented. A bidirectional Transformer trained with masked-modeling losses naturally provides these properties and has become a strong default backbone for sequence reconstruction. We pair it with total-variation regularization to keep \pts\ contiguous and \emph{change-point detection} (\cpd) friendly.
Putting it together, our pipeline learns \pts\ from a single profiling pass by enforcing \emph{group-average matching} (\gam) on retire groups, detects boundaries with exact \pelt\ using autoregressive (AR) segment costs, and preserves budget-matched windows around the detected boundaries. The output is a \emph{keep-list} that composes \emph{within} any SimPoint/LoopPoint region; upstream selection and weights remain unchanged.
\subsection*{Contributions}
\begin{itemize}[leftmargin=*,noitemsep,topsep=1pt]
  \item \textbf{Phase-sensitive pruning within sampled regions.} We formulate within-region pruning as learning a contiguous per-instruction latent (\pts) under retire-group aggregate constraints, so that pruning concentrates around statistically verifiable transitions rather than uniformly thinning instructions.
  \item \textbf{Weakly supervised sequence imputation with \gam.} We train a Transformer imputer whose per-instruction \pts\ satisfies retire-group \emph{mean matching} to standardized anchors. We add smoothing and moment constraints, and provide an affine identifiability result under standardized anchors.
  \item \textbf{Exact segmentation and budget matching.} We run \pelt\ with AR costs for \cpd, followed by an adaptive windowing algorithm that meets a user-specified reduction budget while merging overlaps deterministically.
  \item \textbf{Learning-free fallback.} We provide a contiguous, phase-sensitive surrogate (\S\ref{subsec:beta}) that requires no training yet supports the same \cpd+windowing pipeline.
  \item \textbf{Composability.} Our method is simulator-agnostic and orthogonal to region selection: it operates strictly \emph{within} SimPoint/LoopPoint regions and preserves their selection and weights~\cite{simpoint-asplos02,simpoint03,simpoint-howto,looppoint-hpca22}.
\end{itemize}
% =====================================================================
% Motivation
% =====================================================================
\section{Preliminary Study}
Sampling frameworks (SimPoint, LoopPoint) answer \emph{what} to simulate. VitaBeta is complementary: given a representative trace or checkpoint, \emph{how} can we simulate faster by pruning microarchitecturally redundant instructions while maintaining fidelity?
\subsection*{Trace Pruning Using Analytical Methods}
We first examine analytical pruning rules in ChampSim using SimPoint traces from GAP Benchmark Suite~\cite{beamer2017gapbenchmarksuite}.
\begin{enumerate}[nosep, leftmargin=*]
    \item \textbf{Global Stable Loads (GSLs):} As defined in \textit{Constable}\cite{constable}, these are dynamic load instructions that consistently fetch the same value from the same memory location across multiple instances.  
    \item \textbf{Reuse Distance (RD) Filtering:} This method involves eliminating instructions with small \emph{reuse distances}, a measure of temporal locality that represents the number of unique accesses between two accesses to the same address. The intuition is based on that such instructions access  data that are frequently reused and may have a negligible impact on cache behavior.
    \item \textbf{Footprint-Based Filtering (Footprint):} This approach removes load/store instructions based on memory footprint thresholds, targeting instructions that contribute minimally to the unique set of memory addresses accessed.
    \item \textbf{Value-Leaking Addresses (ValLeak):} As discussed in \emph{CLUELESS}\cite{clueless}, such instructions access memory addresses derived from sensitive data values. ValLeak mainly introduce security vulnerabilities by exposing systems to cache side-channel attacks. { The intuition is based on that ValLeak addresses occur in patterns that are not critical for accurate performance estimation, yet they contribute significantly to trace length.}
\end{enumerate} 
\begin{table}[htbp!]
\caption{Performance of Analytical Methods for Trace Reduction (lower is better for errors; higher is better for speedup).}
\label{tab:comparison_methods}
\centering
\setlength{\tabcolsep}{4pt}
\begin{adjustbox}{max width=1.09\columnwidth}
\begin{tabular}{l|cccc}
\toprule
Metric & GSLs & RD Filtering & Footprint & ValLeak \\
\midrule
IPC error (\%)             & 5.88 & {2.94} & 4.16 & 41.20 \\
Cache miss error (\%)      & \textbf{0.06} & 2.94 & 7.46 & 13.39 \\
Cache latency error (\%)   & \textbf{0.41} & 0.68 & 2.42 & 16.00 \\
Speedup ($\times$)         & 1.06 & 1.08 & 1.06 & \textbf{1.66} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
Table~\ref{tab:comparison_methods} shows that none of them simultaneously achieves substantial speedup while maintaining high accuracy. The following sentence does not feel good, please consider a comprehensive rewrite: "This motivates a signal that correlates with \emph{phase sensitivity} rather than isolated instruction attributes."
Revise the below reasoning carefully and give simple example to make it convincing. Also, you should tailor it to make it suitable for our general story-telling, so that the claim "we need per-instruction level perf metric to do finer-grained trace pruning (secondary sampling) within SimPoint traces" becomes natural like sleep and eat, and thus seems the per-instruction metric is unattainable, we train the ML to learn a surrogate signal instead: "Our objective is to identify portions where significant changes in IPC occur; however, this is only possible through the imputation or prediction of a contiguous IPC series. Modern OoO processors execute instructions concurrently, meaning that the dynamic performance of a program emerges from the collective behavior of the instruction groups rather than from each instruction alone. Consequently, attributing a precise performance measurement (e.g., cycle count) to an individual instruction is inherently ambiguous. Detailed simulators like ChampSim, {logs one cycle count
for an instruction \textit{group} at Reorder Buffer (ROB) retirement and leaves the remaining rows without a unique timing value.} Empirically, only about 30\% of the instructions yield meaningful IPC values.  {We aim to mark such missing IPC entries and reconstruct a contiguous per-instruction IPC sequence to study fine-grained program behaviors} and and prune segments that do not materially impact overall performance.
From Table~\ref{tab:comparison_methods}, we observe that to maintain low error margins in IPC estimation, the fraction of pruned instructions must be closely aligned with the expected simulation speedup. In an ideal case ($\text{IPC}_\text{full}$ = $\text{IPC}_\text{pruned}$), if $X\%$ of the instructions are pruned (retaining $1-X\% $ of the original instructions), the resulting simulation run time should be reduced to approximately  $1-X\%$  of the original, resulting in a $\frac{1}{1-X\%}$ speedup. Othewise, since cumulative IPC is computed as the ratio of total instructions   to total cycles (proportional to execution time)—any discrepancy will directly translate into a corresponding error in the IPC estimation. That is, the pruned instructions chunks should have the collective behavior exhibiting the same IPC measurement as the full trace, if there is no inter-boundary data dependencies. These instructions are the candidates we're trying to find through our proposed approach."
\subsection*{Why not just re-apply SimPoint?}
Region-level resampling selects different minor macroscopic points but still simulates long, internally stable spans and repeatedly pays warmup. Below we provide a case study where we perform SimPoint again on SimPoint traces (Re-SimPoint) to obtain equal trace reduction with \name and compare their performance. Our study shows that, under the same trace reduction, re-applying SimPoint underutilizes the budget (number of instructions after trace pruning) on stable spans and warmup, while \name concentrates simulation where transitions occur, yielding higher speedups at lower error. Conceptually, when budget is tight, spending instructions at phase boundaries buys more fidelity per simulated instruction.
% \name yields a fine-grained latent that is exquisitely sensitive to microarchitectural shifts. 
 
\begin{figure}[!htbp]
    \centering
    \setlength{\belowcaptionskip}{-10pt}
    \captionsetup{skip=0pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \includegraphics[width=0.99\linewidth]{images/resimpoint.pdf}
    \caption{Re-SimPoint refers to baselines that apply fine-tuned SimPoint over again on simulation points (trace regions) obtained through SimPoint to achieve the same trace reduction rate (e.g. 2X, 4X and 10X), you need to study the original paper of SimPoints (```https://jilp.org/vol7/v7paper14.pdf```, ```https://dl.acm.org/doi/10.1145/885651.781076```) to give a detailed, complete and comprehensive parameters settings that can achieve this for GAP benchmarks (e.g. k=? for 2X, 4X, 10X reduction) "Re-SimPoint vs.\ SimPoint+\name\ on GAP Benchmark Suite (single-core). At equal reduction rates (2$\times$, 4$\times$, 10$\times$), \name\ attains greater speedups and substantially lower IPC error. Concentrating the instruction budget around phase boundaries buys more fidelity per simulated instruction than re-sampling coarse regions."}  
    \label{fig:benchmarks}
\end{figure}
\section{Motivation}
\label{sec:motivation}
These sections need a total rewritten. Give the following ascii diagram as the flow we use to compare with LLP (or learning from aggregates): ```
THE IDEAL (UNOBSERVABLE) REALITY
  ---------------------------------
  Inst 1   Inst 2   Inst 3   ...   Inst 99   Inst 100   Inst 101 ...
    |        |        |               |         |          |
 [perf_1] [perf_2] [perf_3] ...   [perf_99] [perf_100] [perf_101]...  <- Ideal per-instruction signal
          +-------------------------------------------+
          |           THE MEASURABLE DATA             |
          +-------------------------------------------+
          |------------ Retire Group G_k -------------|------------ Retire Group G_k+1...
  [Inst 1, Inst 2, ... Inst 100]  [Inst 101, Inst 102, ...]
                  |
                  | (Lossy Compression)
                  V
              [ IPC_k ]                           [ IPC_k+1 ]        <- Aggregate "anchor" signal
              ```
              You should first discuss briefly what LLP is and how it differs with traditional machine learning, and then argue how it helps fit into our problem (LLP + time-series imputation to train a surrogate signal as per-instruction performance metric)
"
\paragraph{Stable spans persist inside representative regions.}
Region-level sampling (SPs/RIDs) answers \emph{what} to simulate. Yet within a chosen region, the microarchitectural response (throughput, queueing, MPKI) often remains in the same regime for long stretches. Simulating every instruction in such spans yields diminishing returns.
\paragraph{Why not “more SPs/RIDs”?}
Re-running the SimPoint method at finer granularity or increasing RIDs fragments regions but does not target the instruction neighborhoods where regime shifts occur. Moreover, repeated resampling repeatedly pays warmup and still traverses stable spans.
\paragraph{Anchored, contiguous latent versus ill-posed instruction timing.}
On OoO cores, cycles are not attributable to single instructions with precision; throughput emerges at the granularity of \emph{retire groups}. We therefore learn a per-instruction surrogate (\pts) whose \emph{group averages} equal a standardized anchor (e.g., group IPC). This sidesteps ill-defined instruction-level timing while preserving aggregate consistency.
\paragraph{A compression lens on supervision.}
Compared to instance labels, retire-group anchors are a \emph{lossy compression}: they discard which instruction contributed what, but retain the group mean. LLP shows that minimizing errors on bag (group) statistics still constrains an instance-level function—especially when bags are \emph{curated} rather than random~\cite{yu2014-llp,scott2020-llp,law2018-agg,zhang2020-agg}. Retire groups are natural, semantically meaningful “bags” tied to the ROB emission process; their proportions (standardized throughput) act as approximate sufficient summaries for our purpose.
\paragraph{Why a Transformer backbone?}
We need a sequence model that (i) denoises noisy anchors, (ii) imputes masked features to encode temporal structure, and (iii) uses long-range context so that extended transitions are not broken into spurious edges. A bidirectional Transformer trained with masked modeling and mild total-variation regularization yields a contiguous, context-aware \pts\ that is well suited for \cpd. In contrast, purely local models tend to over-fragment transitions, and per-instruction regression to anchors overfits to group plateaus.
\paragraph{From \pts\ to actionable pruning.}
Once \pts\ is learned, we detect statistically grounded boundaries with \pelt~\cite{pelt12,ruptures20} using AR costs, then preserve asymmetric windows around each boundary to meet a target reduction. This concentrates budget precisely where phases change, while aggressively pruning stable spans. The result composes with existing region selection pipelines (SimPoint/LoopPoint) without altering their region definitions or weights.
"
\subsection{Key Insights}
\label{sec:intuition}
Our objective is to identify phases where a microarchitectural signal exhibits significant change. On modern OoO processors, IPC emerges from retire \emph{groups}, not single instructions; attributing precise cycles to individual instructions is ambiguous and unnecessary. A naive stepwise signal that forward-fills group throughput to every instruction is too coarse for fine-grained CPD and often misses mid-scale changes.
VitaBeta learns a \emph{per-instruction Phase-Transition Surrogates (PTS)} that (i) preserves aggregate consistency with the observed throughput at the retire-group level and (ii) varies smoothly with local context. CPD on PTS identifies compact, phase-defining neighborhoods. Pruning between those neighborhoods preserves cumulative performance: if the retained segments collectively exhibit throughput similar to the full trace, IPC error remains low while runtime reduces roughly in proportion to the pruned fraction—subject to fixed simulation overheads and warmup costs.
% =====================================================================
% Methodology
% =====================================================================
\section{Methodology}
\label{sec:methodology}
We operate strictly \emph{within} representative regions chosen by SimPoint/LoopPoint. For each region we (1) learn a contiguous per-instruction \emph{Phase-Transition Signal} (\pts) by matching retire-group averages to standardized anchors (single profiling pass), (2) detect change points (\cpd) on \pts\ with exact \pelt, and (3) preserve compact neighborhoods around these boundaries to match a user-specified reduction budget. Upstream region selection and weights remain unchanged.
\subsection{Data, Anchors, and Notation}
Let the dynamic instruction indices in a region be \(T=\{1,\ldots,|T|\}\). Each instruction \(t\) has a feature vector \(u_t\in\mathbb{R}^{D_f}\) (e.g., PC, opcode, address abstractions, locality summaries). The simulator partitions the stream into \emph{retire groups} \(\{\mathcal{G}_k\}_{k=1}^K\) at ROB emission; the mapping \(g(t)=k\) associates instruction \(t\) with its retire group. From a single profiling pass, we observe a \emph{group-level anchor} \(r_k\) (e.g., IPC for group \(k\)). We standardize anchors per stream:
\[
\tilde{r}_k=\frac{r_k-\bar{r}}{\mathrm{sd}(r)},\qquad \bar{r}=\frac{1}{K}\sum_{k=1}^{K}r_k.
\]
\subsection{Phase-Transition Signal (\pts) and Group-Average Matching (\gam)}
A sequence model \(f_\theta\) (Transformer backbone or any strong imputer) emits a scalar \(\pts\) and reconstructed features:
\[
s_t,\,\tilde{u}_t \;=\; f_\theta\!\big(\{u_\tau\}_{\tau\in\mathcal{W}(t)},\,\mathrm{PE}(t)\big),
\]
where \(\mathcal{W}(t)\) is a local or dilated context and \(\mathrm{PE}(t)\) positional encoding. The \emph{group-average matching} constraint enforces aggregate consistency:
\begin{equation}
\mu_k(s)\ \triangleq\ \frac{1}{|\mathcal{G}_k|}\sum_{t\in\mathcal{G}_k}s_t\ \approx\ \tilde{r}_k.
\label{eq:group-avg}
\end{equation}
Intuitively, \(s_t\) is \emph{not} “instruction-level IPC”; it is a contiguous surrogate whose retire-group mean equals the standardized throughput anchor.
\subsection{Transformer Imputer and Weakly Supervised Objective}
We train with masked modeling and aggregate consistency. Concretely, the loss is
\begin{align}
\mathcal{L}_{\mathrm{FR}} &=\frac{\sum_{t,d}M_t^{(d)}\,\ell_d(\tilde{u}_t^{(d)},u_t^{(d)})}{\sum_{t,d}M_t^{(d)}}
\quad\text{(feature reconstruction on observed entries)}, \nonumber\\
\mathcal{L}_{\mathrm{MI}} &=\frac{\sum_{t,d}I_t^{(d)}\,\ell_d(\tilde{u}_t^{(d)},u_t^{(d)})}{\sum_{t,d}I_t^{(d)}}
\quad\text{(masked imputation on artificial masks)}, \nonumber\\
\mathcal{L}_{\mathrm{AC}} &= \frac{1}{|T|}\sum_{k=1}^K |\mathcal{G}_k|\,\rho_\delta\!\big(\mu_k(s)-\tilde{r}_k\big)
\quad\text{(aggregate consistency, Huber)}. \label{eq:ac}
\end{align}
To promote contiguity of \pts\ (for \cpd) and resolve affine ambiguity, we add 
\begin{align}
\mathcal{L}_{\mathrm{TV}} &= \frac{1}{|T|-1}\sum_{t=2}^{|T|} |s_t-s_{t-1}|, \text{(total\ variation)}
% \qquad
% \mathcal{L}_{\mathrm{NORM}} = \big(\bar{s}\big)^2+\big(\Var(s)-1\big)^2, \nonumber
\end{align}
with \(\bar{s}=\tfrac{1}{|T|}\sum_{t=1}^{|T|} s_t\).
The full objective is
\begin{equation}
\mathcal{L} =\mathcal{L}_{\mathrm{FR}} + \lambda\,\mathcal{L}_{\mathrm{MI}} + \gamma\,\mathcal{L}_{\mathrm{AC}} +  \eta\,\mathcal{L}_{\mathrm{TV}}. \label{eq:full}
\end{equation}
\paragraph{Practicalities.}
We train on overlapping tiles (to bound memory), weight retire groups in-batch proportional to counts so that \eqref{eq:ac} approximates the per-instruction normalization, and blend \pts\ in overlaps at inference time. We begin with larger \(\eta\) to avoid jagged \pts, then anneal \(\eta\) so that \pts\ tightens around genuine transitions.
\subsection{Affine Identifiability under Standardized Anchors}
\label{subsec:identifiability}
Assume anchors \(\{\tilde{r}_k\}\) are standardized per stream and training enforces \(\bar{s}\!\approx\!0\) and \(\Var(s)\!\approx\!1\). Under a strictly convex loss near zero in \eqref{eq:ac}, any affine transform \(s'_t=a s_t+b\) that approximately minimizes \(\mathcal{L}_{\mathrm{AC}}\) must satisfy \(a\!\approx\!1\), \(b\!\approx\!0\).
 
\noindent\emph{Sketch.}
Group averaging gives \(\mu_k(s')=a\mu_k(s)+b\). Minimizing \(\sum_k|\mathcal{G}_k|\,\rho_\delta(\mu_k(s')-\tilde{r}_k)\) over \((a,b)\) with the moment constraints yields normal equations whose unique solution is \(a{=}1,b{=}0\) (up to small optimization error) when \(\mathrm{Cov}(\mu(s),\tilde{r})>0\).
\subsection{Change-Point Detection on \pts}
\label{subsec:cpd}
We segment \(s_{1:|T|}\) with \pelt~\cite{pelt12} using an \emph{autoregressive} segment cost of order \(p\) and a BIC-style penalty \(\beta=\alpha\,(p{+}1)\log n\) for tile length \(n\). We sweep a small grid of \(\alpha\) values to obtain candidate boundary sets \(\{\mathcal{C}_j\}\). For each \(\mathcal{C}_j\), we run the budgeted windowing step (next) and select the segmentation whose preserved length is closest to the target (ties broken by fewer segments). This avoids manual penalty tuning while preserving \pelt’s exactness per \(\beta\). For very long traces we apply CPD on overlapping tiles and deduplicate near tile edges.
\subsection{Budgeted Windowing around Detected Boundaries}
Given change points \(\mathcal{C}\subset\{1,\ldots,|T|\}\), we preserve asymmetric windows around each boundary and prune the rest to match target reduction \(r\in(0,1)\). Windows expand or shrink via binary search until the total preserved instructions meet the budget; overlaps are merged.
\begin{algorithm}[t]
\caption{Adaptive Windowing for Budget-Matched Preservation}
\label{alg:adaptive}
\small
\begin{algorithmic}[1]
\Require Change points \(\mathcal{C}\), total length \(|T|\), target reduction \(r\), tolerance \(\epsilon\)
\State \(D\gets \lfloor |T|\,(1-r)\rfloor\); \(W_{\min}\gets 1\); \(W_{\max}\gets |T|\); \(\mathcal{M}^\star\gets\varnothing\); \(P^\star\gets 0\)
\Function{MakeWindow}{$c,W$}
  \State \(\Delta_L\gets\lfloor(W-1)/2\rfloor\), \(\Delta_R\gets W-1-\Delta_L\)
  \State \(s\gets\max(1,c-\Delta_L)\); \(e\gets\min(|T|,c+\Delta_R)\)
  \State \Return \([s,e]\)
\EndFunction
\While{\(W_{\min}\le W_{\max}\)}
  \State \(W\gets\lfloor(W_{\min}+W_{\max})/2\rfloor\)
  \State \(\mathcal{R}\gets \{\textsc{MakeWindow}(c,W):c\in\mathcal{C}\}\); \(\mathcal{M}\gets \textsc{MergeOverlaps}(\mathcal{R})\)
  \State \(P\gets\sum_{[s,e]\in\mathcal{M}}(e-s+1)\)
  \If{\(|P-D|<|P^\star-D|\)} \(\mathcal{M}^\star\gets\mathcal{M};\ P^\star\gets P\) \EndIf
  \If{\(|P-D|\le D\epsilon\)} \Return \(\mathcal{M}\) \EndIf
  \If{\(P < D\)} \State \(W_{\min}\gets W+1\) \Else \State \(W_{\max}\gets W-1\) \EndIf
\EndWhile
\State \Return \(\mathcal{M}^\star\) \Comment{Best-effort if tolerance unmet}
\end{algorithmic}
\end{algorithm}
\subsection{A Learning-Free Surrogate (Beta Metric)}
\label{subsec:beta}
When learning is unavailable, we use a heuristic surrogate that is contiguous and phase-sensitive:
\begin{equation}
\label{eq:beta}
B_t \;=\; \alpha_1\,\ln\big(1+\mathrm{RD}_t\big)\;+\;\alpha_2\,\big|\Delta\mathrm{PC}_t\big|\;+\;\alpha_3\,\big|\Delta r_{g(t)}\big|,
\end{equation}
where \(\mathrm{RD}_t\) is reuse distance, \(\Delta\mathrm{PC}_t\) encodes control-flow change, and \(\Delta r_k=r_k-r_{k-1}\) is the adjacent \emph{group}-anchor gradient, broadcast to instructions in group \(k\). We use \((\alpha_1,\alpha_2,\alpha_3)=(0.5,0.3,0.2)\). CPD and Algorithm~\ref{alg:adaptive} apply unchanged to \(B_t\).
\subsection{Sanity Bounds for Pruning Error}
Suppose CPD partitions \(T\) into segments \(\{\mathcal{S}_j\}\) within which a length-normalized additive performance functional \(F(t)\) varies by at most \(\Delta_j\). If all true regime boundaries lie inside preserved windows and we remove \(N_{\mathrm{pruned}}\) instructions across segments, then
\[
\big|\widehat{F}-F\big|\ \le\ \frac{1}{|T|}\sum_j \Delta_j\, |\mathcal{S}_j\setminus\text{kept}|\ \le\ \Delta_{\max}\frac{N_{\mathrm{pruned}}}{|T|},
\]
so for piecewise-stable behaviors the error scales with the pruned fraction, while preserving transition neighborhoods controls bias.
\subsection{Complexity and Deployment}
Learning uses tiled Transformer inference and batched \gam; \pelt\ is near-linear for fixed penalty families~\cite{pelt12,ruptures20}. The pipeline runs a single profiling pass to extract anchors, trains once per region type or workload family (or uses the learning-free surrogate), and emits keep-lists that downstream simulators apply within existing SPs/RIDs without altering their selection or weights.
  
 
\section{Background and Related Work}\label{background}
Detailed architectural simulation is indispensable for design space exploration, yet end-to-end cycle accuracy across billions of instructions is prohibitively slow. Prior work to accelerate studies falls into two broad families: profile-driven region selection (\emph{sampling}) and \emph{statistical/synthetic} approaches that model or regenerate execution to shorten runs.  Below we position VitaBeta with respect to each family and to recent learning-based techniques. 
\subsection{Sampling of Representative Regions}
SimPoint introduced basic block vectors (BBVs) and clustering to identify representative simulation points that capture large-scale program behavior~\cite{simpoint-asplos02,hamerly-per04,hamerly-jmlr06}. In parallel, SMARTS established statistically rigorous periodic sampling with warmup and confidence bounds for single-threaded applications~\cite{smarts-isca03}. For multi-threaded workloads, several methods leverage structure beyond per-thread BBVs. BarrierPoint identifies inter-barrier regions using synchronization (e.g., OpenMP barriers), and selects representative \emph{barrierpoints} using hybrid code/data signatures (BBVs and LRU stack distance vectors)~\cite{barrierpoint-ispass14}. LoopPoint uses loop-centric profiling to carve repeatable regions, then drives checkpointed sampled simulation in full-system and user modes~\cite{looppoint-hpca22}. Jiang et al.~\cite{jiang2015_taco} proposed a two-level hybrid method that further refines sampling for multi-threaded workloads. More recent work explores online, adaptive sampling (e.g., Pac-Sim) that learns region representativeness at runtime~\cite{pacsimm-taco24}. 
 
These techniques answer \emph{what} macroscopic regions to simulate. \name is complementary: it prunes \emph{within} selected regions using \pts+\cpd to retain only compact neighborhoods around transitions.
In our experiments, we integrate with both SimPoint and LoopPoint without altering their selection mechanisms.
\subsection{Statistical and Synthetic Simulation}
Statistical simulation replaces long detailed runs with synthetic traces or build models from observed distributions of instruction types, dependencies, and memory access patterns, ~\cite{nussbaum-pact01,controlflow-isca04}. \emph{Statistical Flow Graphs}~\cite{wunderlich2003smarts} model control flow and data dependencies to produce representative workloads. {More recent approaches refine this by capturing more complex behaviors}. \emph{Hierarchical Reuse Distance (HRD)}~\cite{hrd} improves cache miss estimation by capturing locality across multiple granularities. Additionally, \emph{Mocktails}~\cite{mocktails} synthesizes spatial-temporal memory patterns for heterogeneous computing devices, thereby bridging the gap between proprietary IP and academic simulation models.  These approaches can be extremely fast, but they often require careful calibration and may under-model local microarchitectural dynamics (e.g., prefetcher  interactions) that matter to modern memory systems.  
\name differs by never generating synthetic program traces: it keeps real instructions near transitions and prunes stable spans.
\subsection{Microarchitectural Metrics}
Locality analysis (e.g., reuse/stack distance) provides powerful summaries of temporal reuse~\cite{mattson1970,benkruskal1975}. Such metrics underpin a broad literature in cache modeling and can help detect regions with limited sensitivity to cache size or replacement. Related to exploiting redundancy, recent microarchitectural proposals identify \emph{likely-stable loads} and eliminate their execution under safety conditions (e.g., Constable)~\cite{constable}. While such mechanisms reduce work during hardware execution, their direct use as pruning rules in trace-driven simulation is brittle: a single instruction-level heuristic rarely correlates with full pipeline and memory-system responses across diverse contexts. 
% This informs redundancy in existing traces but does not directly drive simulation acceleration across diverse pipelines.
% Our preliminary study (Section~\ref{sec:intuition}) shows that standalone heuristics (stable-loads, reuse-distance and footprint filters) are either accurate but low-gain or higher-gain but error-prone. VitaBeta instead seeks a \emph{phase-sensitive} driver that is contiguous in instruction order and anchored to measured performance statistics.
\subsection{Machine Learning for Computer Architecture}
Machine learning has impacted several architectural problems. Neural and RL-based prefetchers model address sequences or decision policies (e.g., Learning Memory Access Patterns~\cite{hashemi2018learning}, Voyager~\cite{voyager-asplos21} and production-grade designs such as SPP~\cite{spp-micro16}, Bingo~\cite{bingo-hpca19}, and Berti~\cite{berti-micro22}). Separately, Ithemal learns basic-block throughput directly from instructions, outperforming analytical models in throughput estimation~\cite{pmlr-v97-mendis19a}. 
On the other hand, methods like TransFetch~\cite{transfetch} and Pythia~\cite{pythia} apply attention-based networks and reinforcement learning, respectively, to generate more accurate memory prefetch requests. Recent work~\cite{mine} investigates the use of large language models for memory trace synthesis.
Our training objective for \pts connects to learning-from-aggregates: LLP and related frameworks learn instance-level predictors from bag-level labels or means~\cite{yu2014-llp,scott2020-llp,law2018-agg,zhang2020-agg}. Here, \emph{retire groups} are bags and group throughput is the regression target.
 
% \subsection{Relation to Concurrent Work and Practical Integration} 
In summary, VitaBeta advances trace reduction by (i) introducing an aggregate-consistent, contiguous per-instruction surrogate for microarchitectural responsiveness; (ii) using principled CPD to delineate compact phase neighborhoods; and (iii) operating orthogonally to established region-selection methods to harvest additional speedups without sacrificing fidelity.
% \section{Motivation}
 
\section{Evaluation on Single-Core Applications} 
\label{evaluation_sc}
We evaluate VitaBeta on reduced SimPoint traces versus full SimPoint traces.
\subsection{Simulation Methodology}
\label{subsec:expsetup}
\textbf{Workloads.} We select memory-intensive traces from SPEC CPU2006~\cite{spec2006}, SPEC CPU2017~\cite{spec2017}, GAP~\cite{beamer2017gapbenchmarksuite}, and ten server workloads collected with gem5~\cite{gem52011} in full-system mode and converted to ChampSim format~\cite{llbp_workloads}. We use reference inputs for SPEC and real/synthetic inputs for GAP.
The ten server traces consist of real-world applications: Eight traces from Java benchmark suites: BenchBase \cite{oltp_bench}, Renaissance \cite{java_renaissance}, and DaCapo \cite{java_dacapo}. Two traces capturing web-server activity: Node.js and PHP-FPM.
% \end{itemize}
\textbf{Environment.} Simulations use a modified ChampSim\cite{ChampSim}, coupled with Ramulator 2.0\cite{luo2023ramulator20modernmodular} for DRAM modeling. We warm up for 10M instructions and collect stats for the next 500M.
\textbf{System configuration.} Table~\ref{tab:baseline} lists a modern OoO baseline. Aggregate anchor signals including IPC (training data) are collected in a fast pass without prefetchers (about 29.27\% faster than prefetcher-enabled simulation), then used to train/apply the PTS model.
\begin{table}[!htbp]
\centering 
\caption{Simulation parameters of the baseline system.}
\begin{tabular}{|l|l|}
\hline
\textbf{Core} & \begin{tabular}[c]{@{}l@{}}
Out-of-order, 4\,GHz \\
8-fetch/decode, 6-dispatch/retire, 512-entry ROB\\
192-entry LQ, 114-entry SQ, 205-entry scheduler
\end{tabular} \\
\hline
\textbf{Caches} & \begin{tabular}[c]{@{}l@{}}
L1I: 32 KB, 64 sets, 8-way, 64B lines, 3 cycles \\
L1D: 48 KB, 64 sets, 12-way, 64B lines, 4 cycles \\
L2: 512 KB, 1024 sets, 8-way, 64B lines, 9 cycles \\
LLC: 2 MB, 2048 sets, 16-way, 64B lines, 40 cycles, LRU 
\end{tabular} \\
\hline
\textbf{BP} & Hashed perceptron predictor~\cite{hashed_perceptron} \\
\hline
\textbf{DRAM} & \begin{tabular}[c]{@{}l@{}}
DDR5-3200, Ramulator 2.0 \\
2 ch, 2 rank/ch, 8 bank groups\\
4 banks/group, 32-bit channel \\
tCAS/tRCD/tRP: 24/24/24, tRAS: 52 \\
Zen4-like mapping
\end{tabular} \\
\hline
\end{tabular}
\label{tab:baseline} 
\end{table}
\textbf{Prefetchers.} We test BERTI~\cite{navarro2022berti}, Bingo~\cite{bingo}, and SPP~\cite{spp}.
\textbf{Model evaluation.} 
We consider:
(i) \emph{pre-trained} PTS models trained on a single representative trace and applied across benchmarks, and
(ii) \emph{application-specific} models trained on the top-weight SimPoints of each application. Unless stated otherwise, results use an ImputeFormer pre-trained on a 10B-instruction \texttt{bfs-10} trace.
We retain SimPoint weights unchanged. For each SimPoint, VitaBeta outputs a reduced trace by concatenating preserved chunks; the final result is the weighted sum over SimPoints.
\subsection{Overhead Analysis}
\label{subsec:overhead}
Let $T_{\text{full}}$ be the time to simulate a full trace once, $T_{\text{prof}}$ the one-time simulation pass used to extract features, and $O$ the remaining one-time overheads (model inference or training, plus CPD+pruning). With reduction factor $r$, each pruned simulation takes $T_{\text{full}}/r$, so the per-run saving versus full simulation is $T_{\text{full}}\!\left(1-\frac{1}{r}\right)$. The break-even number of reduced runs that amortizes the one-time costs is
\begin{equation}
\label{eq:breakeven}
n_{\text{break}} \;\ge\; \frac{r\,(T_{\text{prof}} + O)}{T_{\text{full}}(r-1)}.
\end{equation}
If the profiling pass can be amortized or reused across a design sweep, set $T_{\text{prof}}\!\approx\!0$; otherwise a conservative choice is $T_{\text{prof}}\!\approx\!T_{\text{full}}$. In our setup, typical $T_{\text{full}}$ ranges from $\sim$2\,h (603.bwaves\_s-2609B) to $\sim$6\,h (605.mcf\_s-1644B). 
The one-time overhead beyond profiling is $O{=}0.5$\,h for inference with a pre-trained model or $O{\approx}36$\,h for training from scratch, plus $53$\,min for CPD+pruning (measured on NVIDIA RTX~4090 for learning and Intel Xeon Gold~6338 for simulation/pruning), yielding $O{=}1.38$\,h (pre-trained) or $O{=}36.88$\,h (custom training). As a concrete example, with $r{=}8$ and $T_{\text{full}}{=}5$\,h (bc-12), taking $T_{\text{prof}}{=}T_{\text{full}}$ gives
$n_{\text{break}} \!\ge\! \tfrac{8(5+1.38)}{5\cdot7}\!\approx\!1.46$ (two runs) for the pre-trained case, and
$n_{\text{break}} \!\ge\! \tfrac{8(5+36.88)}{5\cdot7}\!\approx\!9.57$ (about ten runs) when training from scratch. Hence, a pre-trained model yields net benefit after only a few reduced simulations, whereas custom training pays off when many runs are planned.
\subsection{Metrics}
We report the following performance metrics:
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{IPC Error ($E_{\text{IPC}}$)}: The relative error in cumulative IPC between the reduced and full-length trace simulations.
    \item \textbf{Cache Miss Rate Error ($E_{\text{Cache\_miss}}$)}: The relative error in cache miss rates, averaged across all cache levels.
    \item \textbf{Cache Access Latency Error ($E_{\text{Cache\_latency}}$)}: The relative error in average cache access {latency (lat.)}.
    \item \textbf{TLB Miss Rate Error ($E_{\text{TLB\_miss}}$)}: The relative error in miss rates for Translation Lookaside Buffer.
    \item \textbf{TLB Latency Error ($E_{\text{TLB\_latency}}$):} The relative error in averaged TLB access {latency (lat.)}.
    \item \textbf{Simulation Speed-up}: The factor by which the simulation time is reduced compared to the original SimPoint trace.
\end{itemize}
For prefetchers we also report the error of relative IPC improvement, and errors of prefetch accuracy and coverage.
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{IPC Improvement}: The proportional improvement in IPC when the prefetcher is enabled, relative to a no-prefetcher baseline:
    \[
    \text{IPC Improvement} = \frac{\text{IPC}_{\text{with prefetcher}} - \text{IPC}_{\text{no prefetcher}}}{\text{IPC}_{\text{no prefetcher}}}.
    \]
    \item \textbf{Prefetch Accuracy}: The ratio of effective (USEFUL) prefetches to the total number of issued prefetches:
    \[
    \text{Accuracy} = \frac{\text{USEFUL}}{\text{USEFUL} + \text{USELESS}}.
    \]
    \item \textbf{Prefetch Coverage}: The fraction of cache-miss events that are mitigated by effective prefetching, defined as :
    \[
    \text{Coverage} = \frac{\text{USEFUL}}{\text{USEFUL} + \text{LOAD\_MISS} + \text{RFO\_MISS}}.
    \]
\end{itemize}
\begin{figure*}[!htbp] 
    \centering
    \subfloat[Cumulative IPC error (\%).]{
        \includegraphics[width=0.325\textwidth]{individual_boxplot_RDR_IPC_mean_error.pdf}
    }
    \subfloat[Cache miss error (\%).]{
        \includegraphics[width=0.325\textwidth]{individual_boxplot_RDR_cache_miss_error.pdf}
    }
    \subfloat[Cache latency error (\%).]{
        \includegraphics[width=0.325\textwidth]{individual_boxplot_RDR_cache_latency_error.pdf}
    }
    \caption{Error distributions for $2\times$, $4\times$, and $10\times$ reductions on SPEC2006/2017, GAP, and server workloads using a pre-trained PTS model. Average speedups: $1.9\times$, $3.9\times$, $9.2\times$. \textit{\small Note: Unless stated otherwise, results in the section use an ImputeFormer pre-trained on a 10B-instruction \texttt{bfs-10} trace.}}
    \label{fig:distribution_perf}
\end{figure*}
\subsection{Results}
\subsubsection*{General Performance}
Figure~\ref{fig:distribution_perf} shows that about 75\% of traces have IPC error below 5\% at $2\times$ reduction; this increases modestly at $10\times$ while speedups scale sublinearly with the reduction factor due to fixed costs. Outliers are bandwidth-bound kernels (e.g., \texttt{GemsFDTD}, \texttt{bwaves}, \texttt{roms}, \texttt{mcf}) where small misalignment in preserved segments can amplify cache errors; even then, IPC errors generally remain below 17.5\%.
Overall, the boxplots confirm that VitaBeta maintains traces of their architectural fidelity across diverse application domains, with only a handful of predictable outliers, and that performance gains scale almost proportionally with trace-length reduction.
\begin{figure}[!htbp]
    \centering
    \setlength{\belowcaptionskip}{-10pt}
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \includegraphics[width=0.99\linewidth]{images/overall_performance/error_metrics_speedup_benchmark_comparison.pdf}
    \caption{Benchmark-level performance with a pre-trained PTS model at $2\times$ reduction. Error bars: standard deviation.}
    \label{fig:benchmarks}
\end{figure}
\subsubsection*{Across Benchmark Suites}
Figure~\ref{fig:benchmarks} shows SPEC2017 attains the lowest cache errors; GAP exhibits higher IPC and TLB-related errors due to irregular memory behavior and large footprints, yet still benefits from reduction.
\begin{figure*}[!htbp]
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_trace_reduction_rate.pdf}
        \caption{Relative IPC improvement error (\%).}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_on_accuracy_vs_trace_reduction_rate.pdf}
        \caption{Prefetch accuracy error (\%).}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_on_coverage_vs_trace_reduction_rate.pdf}
        \caption{Prefetch coverage error (\%).}
    \end{subfigure} 
    \caption{Relative prefetcher accuracy on pruned traces (pre-trained PTS model), averaged over SPEC2006/2017, GAP, and server workloads. Mean IPC improvements for full-traces: BERTI 20.28\%, Bingo 27.80\%, SPP 34.50\%.}
    \label{fig:prefetch_perf}
\end{figure*}
\begin{figure*}[!htbp] 
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_benchmark_2X_rdr.pdf}
        \caption{IPC improvement error at $2\times$.}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_benchmark_4X_rdr.pdf}
        \caption{IPC improvement error at $4\times$.}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_benchmark_10X_rdr.pdf}
        \caption{IPC improvement error at $10\times$.}
    \end{subfigure} 
    \caption{Prefetcher IPC-improvement errors across benchmark suites (pre-trained PTS model).}
    \label{fig:prefetch_benchmark}
\end{figure*}
\subsubsection*{Relative Prefetcher Performance}
Figure~\ref{fig:prefetch_perf} shows that the ordering among BERTI, Bingo, and SPP is preserved up to $4\times$ reduction and remains informative at $10\times$ for workloads with larger baseline gains (e.g., GAP). 
% Prefetch accuracy and coverage errors stay below $\sim$1–1.5\%.
 At higher reduction rates from (\(8\times\) to \(10\times\)), however, the error can exceed half the difference between the IPC improvements of any two prefetchers and the results may not be reliable. This effect is particularly pronounced for benchmarks with low baseline IPC improvements. For example, Figure~\ref{fig:prefetch_benchmark} shows that server traces, with a baseline improvement of only 7.64$\%$, exhibit relative errors of 1.41\%, 3.17\%, and 3.95\% under \(2\times\), \(4\times\), and \(10\times\) reductions, respectively. In contrast, benchmarks with higher baseline IPC improvements (e.g., GAP with 63.32\%) maintain a robust relative accuracy of prefetchers even at higher reduction rates.
 
Additionally, prefetch accuracy and coverage errors stay below $\sim$1–1.5\% for trace reductions between \(2\times\) and \(10\times\). These metrics are retained very similar   between both sets of runs and help prefetcher designer make the right analysis and conclusions
\begin{figure*}[!htbp]
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{server_2x_performance_comparison.pdf}
        \caption{$2\times$ reduction.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{server_4x_performance_comparison.pdf}
        \caption{$4\times$ reduction.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{server_10x_performance_comparison.pdf}
        \caption{$10\times$ reduction.}
    \end{subfigure}
 \caption{Performance on server workloads with various prefetchers: metric errors vs. reduction (pre-trained PTS model).}
    \label{fig:abs_prefetch_perf}
\end{figure*}
\subsubsection*{Server Workloads}
Figure~\ref{fig:abs_prefetch_perf}  summarizes the performance of VitaBeta with 3 different prefetchers  on 10 server workloads. It shows most metrics remain within $\le$10\% error up to $10\times$ reduction, with speedups saturating near $9.4\times$ due to fixed costs. TLB miss errors are higher than SPEC/GAP at aggressive reductions, reflecting large, locality-optimized heaps and GC behavior that make TLB dynamics sensitive to trace pruning.
\subsubsection*{Branch Prediction Accuracy}
Because VitaBeta preserves sizable chunks around change points, branch distributions remain close to the originals. At $2\times$ reduction with a pre-trained model, BP accuracy errors remain small (e.g., 0.126\% SPEC2006, 0.117\% SPEC2017, 0.489\% GAP) despite significant instruction pruning in GAP.
\begin{figure*}[!htbp]  
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \includegraphics[width=.99\textwidth]{publication_subplots_hatched_bars.pdf}
    \caption{Different Backbone Models for generating PTS: Beta heuristics, non-Transformers, and Transformer variants at $4\times$ reduction (no prefetchers enabled). Error bars: standard deviation.}
    \label{fig:model_perf}
\end{figure*}

 
\subsection{Sensitivity Analysis}
\subsubsection*{Model Choices}
Figure~\ref{fig:model_perf} compares Various PTS backbones. Transformer variants (SAITS/ImputeFormer) outperform non-Transformers (TEFN~\cite{TEFN}, ModernTCN~\cite{moderntcn}) and the heuristic Beta Metric, especially at higher reductions. Application-specific training offers up to $\sim$10\% additional error reduction over a strong pre-trained model; heavier models (TimesNet, TimeMixer) do not consistently outperform compact Transformers given similar training budgets.
\subsubsection*{Impact of Training Data}
Table~\ref{tab:sensitivity_training_onecol} indicates that pre-training on different representative traces yields similar aggregate accuracy; \texttt{gcc-13B} pre-training slightly improves means but increases variance due to its chaotic phase structure~\cite{Shen+:ASPLOS04}. % CITATION NEEDED
\begin{table}[htbp!]
  \caption{Different Pre-Training Traces (SPEC2017/2006/GAP), $2\times$ reduction. Prefetcher: BERTI. Values are geometric means in \%. Lower is better except Speedup.}
  \label{tab:sensitivity_training_onecol}
  \centering
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{max width=1.0\columnwidth}
  \begin{tabular}{l|rrrrrr}
  \toprule
  Training data & ${E}_{\text{IPC}}$  & $E_{\text{cache\_miss}}$  &$E_{\text{cache\_latency}}$  & $E_{\text{TLB\_miss}}$    & $E_{\text{TLB\_latency}}$   & Speedup \\
  \midrule
  astar-313B & \textbf{3.224\%} & 2.625\% & \textbf{1.783\%} & 3.655\% & 4.213\% & $1.971\times$ \\
  bfs-10     & 3.458\% & \textbf{2.373\%} & 1.926\% & 3.283\% & 4.498\% & $1.952\times$ \\
  gcc-13B    & 3.229\% & 2.456\% & 2.094\% & \textbf{2.787\%} & \textbf{4.190\%} & \textbf{$1.989\times$} \\
  \bottomrule
  \end{tabular}
  \end{adjustbox}
  \vspace{0.25ex}
  \\
  \footnotesize Note: Same ImputeFormer hyperparameters; models are pre-trained on different traces and evaluated across the suites.
\end{table}
\subsubsection*{CPD Cost Functions}
We conduct experiments comparing the {Auto-Regressive ($AR$)} cost function with the {Least Squared Deviation ($L_2$)} cost function. We assess their effectiveness on \textit{TimeMixer} and \textit{TimesNet}, since these two models tend to yield very high variance among all error metrics compared to other Transformer-based models. The $L_2$ cost function is defined as: $$c_{\text{$L_2$}}(y_{t_a:t_b}) = \sum_{t = t_a}^{t_b} \left( y_t - \bar{y}_{t_a:t_b} \right)^2,$$
where $\bar{y}_{t_a:t_b}$ is the mean of the segment $[t_a, t_b]$. 
While $AR$ is suitable for time series where current values are influenced by past values, the $L_2$ cost function is effective for detecting shifts in the mean level of the signal\cite{Lavielle2005}.   
Table~\ref{tab:sensitivity_cpd_onecol} shows TimeMixer prefers AR cost (temporal dependence), while TimesNet prefers $L_2$ (mean shifts). Selecting a CPD cost aligned with the backbone’s inductive bias improves stability.
\begin{table}[htbp!]
  \caption{CPD cost functions with different backbones (SPEC2017/2006/GAP), $2\times$ reduction. Prefetcher: BERTI. Values are geometric means in \%. Lower is better except Speedup.}
  \label{tab:sensitivity_cpd_onecol}
  \centering
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{max width=1.0\columnwidth}
  \begin{tabular}{l|rrrrrr}
  \toprule
  Approach &  $E_{\text{IPC}}$  & $E_{\text{cache\_miss}}$  &$E_{\text{cache\_latency}}$  & $E_{\text{TLB\_miss}}$    & $E_{\text{TLB\_latency}}$   & Speedup\\
  \midrule
  \footnotesize TimeMixer + AR-CPD    & \textbf{2.175\%} & \textbf{1.249\%} & \textbf{1.400\%} & \textbf{2.702\%} & \textbf{3.207\%} & $1.959\times$ \\
  \footnotesize TimeMixer + $L_2$-CPD  & 2.870\% & 1.392\% & 1.419\% & 3.313\% & 4.234\% & \textbf{$2.035\times$} \\
  \midrule
  \footnotesize TimesNet + AR-CPD     & 3.514\% & 1.924\% & 2.675\% & 3.217\% & 4.223\% & $1.995\times$ \\
   \footnotesize TimesNet + $L_2$-CPD     & \textbf{3.027\%} & \textbf{1.789\%} & \textbf{2.356\%} & \textbf{3.495\%} & \textbf{3.720\%} & $1.987\times$ \\
  \bottomrule
  \end{tabular}
  \end{adjustbox}
  \vspace{0.25ex}
  \\
  \footnotesize Note: Each Backbone model share the same hyperparameters; only the CPD cost differs.
\end{table}
\section{Evaluation on Multi-Core Applications} 
\label{evaluation_mc}
\textbf{Experimental setup.} We follow LoopPoint in gem5 full-system~\cite{LoopPointTutorialHPCA23} on seven NAS Parallel Benchmarks~\cite{npb_benchmark} (Class A~\cite{npb_input_size}, 4 OpenMP threads) with a Skylake-like quad-core (Table~\ref{tab:detailed_board}) baseline configuration.
\begin{table}[!htbp]
\centering
\caption{Configuration of detailed gem5 FS simulation.}
\begin{tabular}{|l|l|}
\hline
\textbf{CPU Model} & Out-of-order, 4\,GHz, SkyLakeCPU (4 cores) \\ 
\hline
\textbf{Caches} & \begin{tabular}[c]{@{}l@{}}
L1I/L1D: 64 KB, 8-way;\; L2: 2 MB, 64-way \\
Coherence: MESI two-level
\end{tabular} \\
\hline
\textbf{System} & Ubuntu 24.04 (4 CPUs, built with QEMU~\cite{qemu}) \\
\hline
\textbf{DRAM} & DDR4\_2400\_16x4, 3GB \\
\hline
\end{tabular}
\label{tab:detailed_board}
\end{table}
\vspace{-0.5cm}
\subsection{LoopPoint Workflow}
\begin{enumerate}[nosep, leftmargin=*]
  \item \textbf{Boot Checkpointing.}  
        We fast-forward a Ubuntu 24.04 image to the shell prompt with a
        \textit{X86KvmCPU} and save an \emph{after-boot} snapshot.
        This removes the OS boot path from all subsequent experiments.
  \item \textbf{Process-map Extraction.}  
        Restoring the boot image we launch the
        NPB binary and dump
        \emph{/proc/\$PID/maps}.  
        We filter the map to get (i) the application
        text segment---used later to validate marker PCs—and
        (ii) ``unsafe'' ranges (OpenMP, {pthread} etc.) that must
        be \emph{excluded} from BBV collection. 
  \item \textbf{LoopPoint analysis.}  
         We enable a \emph{LooppointAnalysis} probe that listens       inside the application’s text range; library addresses found in
        the previous step are masked out.  
        Region length  is fixed to 400M instructions.  
        We record the BBV, global instruction count and loop
        counters and then reset all statistics. 
        A typical NPB-A
        run produces $50$–$90$ regions, which are later clustered using k-means for representative  selection.         For every representative region ID (RID) we derive three
        markers (warm-up, start, end).
 
  \item \textbf{Checkpoint construction.}  
        Starting from the boot snapshot,
        we enable a
        \textit{PcCountTracker} that listens at
        \textit{WORKBEGIN}, and dumps a checkpoint whenever a warm-up or
        start marker fires.  
        Regions whose warm-up coincides with
        \textit{WORKBEGIN} are captured automatically, ensuring \emph{one
        micro-checkpoint (LoopPoint) per RID} and zero redundant state.
  \item \textbf{Detailed replay.}  
        We restores each
        LoopPoint on the detailed core
        (Tab.~\ref{tab:detailed_board}).  
        Per-region stats are scaled by the cluster weight and summed;
        the grand total is compared against an unsampled detailed run.
\end{enumerate}
\subsection{Using VitaBeta  for {LoopPoint Refinement}}
\begin{enumerate}[nosep, leftmargin=*]
\item \textbf{Generate elastic traces.}  
For each application profiled by LoopPoint, we load information from the same RID and use a modified \textit{TraceCPU} to  emit an \textit{interleaved trace} similar to Table~\ref{tab:example} from gem5 elastic traces\cite{elastic_trace_gem5} (memory data trace and instruction fetch trace).
\item \textbf{Offline pruning to generate new checkpoints.}  
    We then run VitaBeta with a target $4\times$ reduction to retains only the
      instruction spans around its detected change points. This results in a compact \emph{keep-list}:
      \(\langle\text{PC},\,\Delta\text{inst}\rangle\) pairs that delimit the surviving slices. Finally, we re-checkpoint in the same way as LoopPoint with the generated \textit{PcCountTracker}. Detailed replay on these checkpoints uses the same core configuration (Table~\ref{tab:detailed_board}).
\end{enumerate} 
\begin{figure*}[!htbp] 
    \centering
    \includegraphics[width=1.00\textwidth]{lp2.pdf}
    \caption{Per-benchmark breakdown for LoopPoint+VitaBeta (pre-trained PTS).}
    \label{fig:lp_vb_breakdown} 
\end{figure*}
\begin{figure}[!htbp] 
    \centering
    \includegraphics[width=.99\columnwidth]{lp1.pdf}
    \caption{LoopPoint vs.\ LoopPoint+VitaBeta (against unsampled detailed ground truth).}
    \label{fig:lp_vb_comparison} 
\end{figure}
% \vspace{-0.5cm}
\subsection{Results}
We evaluate three configurations:
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{Ground-truth (GT): }  full, unsampled detailed run.
    \item \textbf{LoopPoint (LP): }  vanilla LoopPoint, average 30 regions.
    \item \textbf{LP+VB: } VitaBeta-refined LoopPoints ($4\times$ reduction).
\end{itemize}
\smallskip\noindent
We sum per-region statistics weighted by region
multiplicity and report the GeoMean error of
\textsc{LP}/\textsc{LP+VB} versus \textsc{GT}.  
Figure~\ref{fig:lp_vb_comparison} shows that \textsc{LP+VB} multiplies
LoopPoint’s original $4.3\times$ speed-up to {23.9$\times$}—a $5.6\times$ additional acceleration,while IPC error rises modestly from $3.1\%$ to {9.6\%}.  Cache and DRAM latency errors remain below $10.5\%$ and $17.0\%$,
respectively.  Although VitaBeta’s pruning is uniformly aggressive, accuracy deteriorates sub-linearly corresponding to the gain in speed-up. Figure~\ref{fig:lp_vb_breakdown} indicates compute-bound workloads
(\textit{CG}, \textit{EP}) gain the most ($42$–$285\times$ speed-up),
whereas memory-intensive ones with irregular spatial locality (\textit{MG}, \textit{LU}) incur 
highest DRAM-latency error.  These high errors are caused by interleaving the multi-threading traces that coincides with page-table walks and conflict-miss bursts. Nevertheless, even for them, the IPC error stays below 20\%, and the simulator still runs an order of magnitude faster than  LoopPoint baseline. 
These results validate VitaBeta as an \emph{orthogonal}
complement to region-level sampling in multi-core full-system studies.  
 
 
\end{document}
