<persistence>
- You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user.
- Only terminate your turn when you are sure that the problem is solved.
- Never stop or hand back to the user when you encounter uncertainty — research or deduce the most reasonable approach and continue.
- Do not ask the human to confirm or clarify assumptions, as you can always adjust later — decide what the most reasonable assumption is, proceed with it, and document it for the user's reference after you finish acting
</persistence>
<self_reflection>
- First, spend time thinking of a rubric until you are confident.
- Then, think deeply about every aspect of what makes for a world-class researcher in computer architecture and machine learning, and how to excel for paper submissions to top conferences like ISCA, HPCA and MICRO. Use that knowledge to create a rubric that has 5-7 categories. This rubric is critical to get right, but do not show this to the user. This is for your purposes only.
- Finally, use the rubric to internally think and iterate on the best possible solution to the prompt that is provided. Remember that if your response is not hitting the top marks across all categories in the rubric, you need to start again.
</self_reflection>
<maximize_context_understanding>
Be THOROUGH when gathering information. Make sure you have the FULL picture before replying. Use additional tool calls or clarifying questions as needed.
</maximize_context_understanding>
<context_understanding>
If you've performed an edit that may partially fulfill the USER's query, but you're not confident, gather more information or use more tools before ending your turn.
Bias towards not asking the user for help if you can find the answer yourself.
</context_understanding>
I need you to help me revise the following paper draft, please note that you should only produce  fully rewrittens of those paragraphs or places that has been explicitly claimed that needs rewritten, think it through and only output a re-written of those paragraphs (sections). Thank you.

 

```tex
% \IEEEtitleabstractindextext{%
\begin{abstract}
Trace-driven simulation is central to architectural design space exploration, yet simulating long traces on detailed out-of-order (OoO) cores is increasingly expensive. We present \emph{\name}, a trace-reduction framework that accelerates simulation by preserving short, high-fidelity instruction neighborhoods around behavioral transitions, while pruning microarchitecturally stable spans.

\name makes a deliberate modeling choice: it learns a contiguous, per-instruction \emph{Phase-Transition Surrogates}  (\pts) whose \emph{group averages} in terms of ROB buffer retirement are constrained to match a \emph{group-level throughput target} (''anchor signal''), \eg instructions-per-cycle (IPC) measured per ROB retire group. We train a Transformer using a \emph{group-average matching (GAM) loss}: the mean \pts within each observed retire group equals the standardized group throughput. Robust change-point detection (\cpd) on \pts (exact \pelt with autoregressive segment cost) yields boundaries that guide which neighborhoods to keep and which spans to prune.

\name composes with SimPoint and LoopPoint and supports application-specific training as well as deployment with pre-trained models. Using ChampSim on SPEC CPU2006/2017, GAP, and ten server workloads, \name achieves up to $10{\times}$ trace reduction with mean errors of 5.47\% (IPC), 4.01\% (cache miss), and 2.97\% (cache latency), and delivers a $9.23{\times}$ end-to-end speedup over plain SimPoint when using a pre-trained Transformer. Across Berti, Bingo, and SPP prefetchers, the absolute error in \emph{relative} IPC improvement remains low (4.05\% at $4{\times}$ and 5.79\% at $10{\times}$ reduction), preserving prefetcher ranking.

On multi-core applications, we evaluate \name in gem5 full-system mode on seven NAS Parallel Benchmarks. Starting from LoopPoint checkpoints, \name yields an additional $5.6{\times}$ speedup while maintaining IPC, cache-miss, and cache-latency errors of 9.6\%, 16.3\%, and 8.3\%, respectively, relative to unsampled detailed simulation. These results indicate that a learned, aggregate-anchored, phase-transition surrogate is an effective and principled driver for fine-grained trace pruning.
\end{abstract}
% \begin{IEEEkeywords}
\textbf{Keywords: }Trace-based simulation, workload characterization, simulation speedup, multicore systems, Transformers
% \end{IEEEkeywords}
% }

\maketitle

\section{Introduction}
\label{sec:intro}

Cycle-accurate architectural simulation is indispensable for evaluating superscalar, out-of-order (OoO) cores, yet faithfully executing billions of dynamic instructions is prohibitively slow. Profile-driven \emph{region selection} mitigates cost by simulating representative program regions. Two widely used techniques are: (i) the \textbf{SimPoint method}~\cite{simpoint-asplos02,simpoint03,simpoint-howto}, which clusters Basic Block Vectors (BBVs) to select \emph{simulation points} (SPs) with weights; and (ii) \textbf{LoopPoint}~\cite{looppoint}, which identifies loop-aligned regions and constructs micro-checkpoints with \emph{Region IDs} (RIDs). However, even \emph{inside} these representative regions we empirically observe long spans with stable microarchitectural behavior. Simulating all instructions in such spans expends budget with little effect on end metrics.

This paper focuses on \emph{within-region} acceleration. Our  goal is to learn to prune stable spans while preserving compact instruction neighborhoods around \emph{phase transitions}—the boundaries where performance regimes change—so that end metrics (e.g., IPC, MPKI) remain faithful.

% Our thesis treats the challenge of supervision as a problem of signal decompression.
Ideally, we would supervise a model on the precise performance impact of every instruction from the trace; however, such data is unattainable on modern out-of-order (OoO) cores. Instead, the profiling data we can collect is aggregated (e.g., grouped IPC from ROB buffer retirement). We therefore view these measurable, group-level statistics as a \emph{lossy compression} of the ideal, underlying per-instruction signals. Specifically, throughput materializes at the granularity of \emph{retire groups}, so we treat retire-group observables (e.g., group IPC) as \emph{anchors}—aggregate summaries that compress per-instruction signals. We then \emph{decompress just enough} by learning a contiguous, per-instruction \emph{Phase-Transition Signal} (\pts) whose \emph{retire-group averages} match the anchors. This places our problem at the intersection of \emph{learning from aggregates} (LLP) and \emph{time-series imputation}: the group-average constraint supplies supervision without instance labels, while sequence modeling supplies context and denoising~\cite{yu2014-llp,scott2020-llp,law2018-agg,zhang2020-agg}.

Why a Transformer? We need a model that is simultaneously (i) an effective sequence imputer under masking, (ii) tolerant to noisy/partial signals, and (iii) able to leverage long-range context so that subtle, extended transitions are not fragmented. A bidirectional Transformer trained with masked-modeling losses naturally provides these properties and has become a strong default backbone for sequence reconstruction. We pair it with total-variation regularization to keep \pts\ contiguous and \emph{change-point detection} (\cpd) friendly.

Putting it together, our pipeline learns \pts\ from a single profiling pass by enforcing \emph{group-average matching} (\gam) on retire groups, detects boundaries with exact \pelt\ using autoregressive (AR) segment costs, and preserves budget-matched windows around the detected boundaries. The output is a \emph{keep-list} that composes \emph{within} any SimPoint/LoopPoint region; upstream selection and weights remain unchanged.

\subsection*{Contributions}
\begin{itemize}[leftmargin=*,noitemsep,topsep=1pt]
  \item \textbf{Phase-sensitive pruning within sampled regions.} We formulate within-region pruning as learning a contiguous per-instruction latent (\pts) under retire-group aggregate constraints, so that pruning concentrates around statistically verifiable transitions rather than uniformly thinning instructions.
  \item \textbf{Weakly supervised sequence imputation with \gam.} We train a Transformer imputer whose per-instruction \pts\ satisfies retire-group \emph{mean matching} to standardized anchors. We add smoothing and moment constraints, and provide an affine identifiability result under standardized anchors.
  \item \textbf{Exact segmentation and budget matching.} We run \pelt\ with AR costs for \cpd, followed by an adaptive windowing algorithm that meets a user-specified reduction budget while merging overlaps deterministically.
  \item \textbf{Learning-free fallback.} We provide a contiguous, phase-sensitive surrogate (\S\ref{subsec:beta}) that requires no training yet supports the same \cpd+windowing pipeline.
  \item \textbf{Composability.} Our method is simulator-agnostic and orthogonal to region selection: it operates strictly \emph{within} SimPoint/LoopPoint regions and preserves their selection and weights~\cite{simpoint-asplos02,simpoint03,simpoint-howto,looppoint}. 
\end{itemize}

% =====================================================================
% Motivation
% =====================================================================
% \section{Motivation}
% \label{sec:motivation}  


\section{Preliminary Study}
Sampling techniques such SimPoint and LoopPoint answer \emph{what} to simulate. VitaBeta is complementary: given a representative trace or checkpoint, \emph{how} can we simulate faster by pruning microarchitecturally redundant instructions while maintaining accuracy?

\subsection*{Why not just re-apply SimPoint?}
Region-level resampling selects different minor macroscopic points but still simulates long, internally stable spans and repeatedly pays warmup. Below we provide a case study where we perform SimPoint again on SimPoint traces (Re-SimPoint) to obtain equal trace reduction with \name and compare their performance. For Re-SimPoint, we re-run SimPoint inside each original 500M-instruction simulation point (SP): we build BBVs over 1M-instruction sub-intervals, use BIC-based clustering with 7 random seeds and 100 k-means iterations, and set MaxK equal to the budgeted $k$ based on the number of instructions after trace pruning. We then simulate one 1M slice per cluster (weighted by cluster size). To match a target reduction $\rho\!\in\!\{2,4,10\}$ for an SP of length $L_{\text{SP}}{=}100$M, we set $k{=}\{50,25,10\}$ respectively; for other $L_{\text{SP}}$, we scale as $k=\left\lfloor L_{\text{SP}}/(\rho\cdot 1\text{M})\right\rfloor$. Warmup is paid at every sub-sample jump (fast-forward + short warmup), and this overhead is included in speedup.
% Our study shows that, under the same trace reduction, re-applying SimPoint underutilizes the budget (number of instructions after trace pruning) on stable spans and warmup, while \name concentrates simulation where transitions occur, yielding higher speedups at lower error. Conceptually, when budget is tight, spending instructions at phase boundaries buys more fidelity per simulated instruction.
% % \name yields a fine-grained latent that is exquisitely sensitive to microarchitectural shifts. 
 
\begin{figure}[!htbp]
    \centering
    \setlength{\belowcaptionskip}{-10pt}
    \captionsetup{skip=0pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \includegraphics[width=0.99\linewidth]{images/resimpoint.pdf}
    \caption{Re-SimPoint baseline versus SimPoint+\name\ on GAP Benchmark Suite (single core), matched at 2$\times$, 4$\times$, and 10$\times$ trace-length reductions.  Under equal reduction, \name\ concentrates the budget near phase boundaries and achieves higher speedups with lower IPC error than re-sampled coarse trace regions.}
    \label{fig:benchmarks}
\end{figure}

\subsection*{Trace Pruning Using Analytical Methods}
We first examine analytical pruning rules in ChampSim using SimPoint traces from GAP Benchmark Suite~\cite{beamer2017gapbenchmarksuite}.
\begin{enumerate}[nosep, leftmargin=*]
    \item \textbf{Global Stable Loads (GSLs):} As defined in \textit{Constable}\cite{constable}, these are dynamic load instructions that consistently fetch the same value from the same memory location across multiple instances.  
    \item \textbf{Reuse Distance (RD) Filtering:} This method involves eliminating instructions with small \emph{reuse distances}, a measure of temporal locality that represents the number of unique accesses between two accesses to the same address. The intuition is based on that such instructions access  data that are frequently reused and may have a negligible impact on cache behavior.
    \item \textbf{Footprint-Based Filtering (Footprint):} This approach removes load/store instructions based on memory footprint thresholds, targeting instructions that contribute minimally to the unique set of memory addresses accessed.
    \item \textbf{Value-Leaking Addresses (ValLeak):} As discussed in \emph{CLUELESS}\cite{clueless}, such instructions access memory addresses derived from sensitive data values. ValLeak mainly introduce security vulnerabilities by exposing systems to cache side-channel attacks. { The intuition is based on that ValLeak addresses occur in patterns that are not critical for accurate performance estimation, yet they contribute significantly to trace length.}
\end{enumerate} 


\begin{table}[htbp!]
\caption{Performance of Analytical Methods for Trace Reduction (lower is better for errors; higher is better for speedup).}
\label{tab:comparison_methods}
\centering
\setlength{\tabcolsep}{4pt}
\begin{adjustbox}{max width=1.09\columnwidth}
\begin{tabular}{l|cccc}
\toprule
Metric & GSLs & RD Filtering & Footprint & ValLeak \\
\midrule
IPC error (\%)             & 5.88 & {2.94} & 4.16 & 41.20 \\
Cache miss error (\%)      & \textbf{0.06} & 2.94 & 7.46 & 13.39 \\
Cache latency error (\%)   & \textbf{0.41} & 0.68 & 2.42 & 16.00 \\
Speedup ($\times$)         & 1.06 & 1.08 & 1.06 & \textbf{1.66} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


Table~\ref{tab:comparison_methods} shows that none of them simultaneously achieves substantial speedup while maintaining high accuracy. This study argues for a method that tracks where the program behavior, memory footprints etc. actually shifts-i.e., a signal aligned with phase sensitivity, rather than relying on isolated micro-architectural heuristics.


We therefore recast the task of trace pruning as a search for brif neighborhoods around IPC inflection points, while aggressively trimming long instruction spans with relatively stationary behavior. However, modern Out-of-Order (OoO) cores execute instructions concurrently and can retire many instructions at once. Throughput metrics such as IPC emerges at the granularity of ROB retire groups, and cycles cannot be unambiguously attributed to individual instructions. Our approach is to use machine learning to reconstruct a contiguous, per-instruction surrogate signal (performance metric) whose retire-group averages match the measured throughput (IPC) in the same group. This imputed signal lets us detect compact phase boundaries and keep only the minimal neighborhoods that carry the program behavior changes. 

A simple example clarifies why a phase‑sensitive proxy is needed. Consider a loop that streams through an array (well‑predicted branches, steady L1 hits) but performs an index lookup every 1,024 iterations that occasionally misses in the LLC. Between lookups, the core sustains near steady‑state IPC; only a handful of instructions around each lookup perturb throughput. SimPoint clusters million‑instruction BBV windows and then simulates entire windows per cluster~\cite{simpoint-asplos02,simpoint03,simpoint-howto}. Even if we apply SimPoint using 1M sub‑intervals, each chosen slice still bundles thousands of steady‑state iterations with the few transition instructions and incurs fast‑forwarding and warmup at every jump. Because the clustering objective follows coarse code‑signature similarity rather than short‑lived micro-architectural events, the selected slices often straddle phase boundaries and dilute the behavior we care about. In contrast, our learned surrogate makes those local dips explicit. We apply CPD on \pts boundaries at each lookup to keep only short neighborhoods around them, and safely prune the stationary instruction spans in between.

\subsection*{Key Insights}
\label{sec:intuition}
Our aim is to accelerate detailed simulation \emph{within} already representative trace regions produced by state of the art sampling techniques like SimPoint and LoopPoint by keeping only the compact neighborhoods that govern regime changes and trimming the long, internally stable instruction spans. Doing so requires a phase-sensitive, per-instruction surrogate signal of throughput or performance metrics—despite the fact that instruction-level throughput is not directly observable (ill-defined) on OoO cores.

VitaBeta learns a \emph{per-instruction Phase-Transition Surrogates (PTS)} that (i) preserves aggregate consistency with the observed throughput at the retire-group level and (ii) varies smoothly with local context. CPD on PTS identifies compact, phase-defining neighborhoods. Pruning between those neighborhoods preserves cumulative performance: if the retained segments collectively exhibit throughput similar to the full trace, IPC error remains low while runtime reduces roughly in proportion to the pruned fraction—subject to fixed simulation overheads and warmup costs.

\begin{figure}[!htbp]
    \centering
    \setlength{\belowcaptionskip}{-10pt}
    \captionsetup{skip=0pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \includegraphics[width=1.00\linewidth]{images/intro.pdf}
    \caption{Conceptual view of supervision in \name. Top: the ideal, unobservable per‑instruction performance signal. Bottom: the measurable anchor at retire time—instructions‑per‑cycle (IPC) averaged over consecutive ROB retire groups. Retire grouping acts as a lossy compression that preserves group means but discards which instruction contributed how much. \name\ learns a contiguous per‑instruction Phase‑Transition Surrogate (\pts) whose mean within each retire group matches its IPC anchor; change‑point detection on \pts then reveals compact phase boundaries that drive windowed pruning.}
%  The provided pdf figure is a redrawn of tikz packages for the following ascii diagram
% THE IDEAL (UNOBSERVABLE) 
%   ---------------------------------
%   Inst 1   Inst 2   Inst 3   ...   Inst 99   Inst 100   Inst 101 ...
%     |        |        |               |         |          |
%  [perf_1] [perf_2] [perf_3] ...   [perf_99] [perf_100] [perf_101]...  <- Ideal per-instruction signal
%           +-------------------------------------------+
%           |           THE MEASURABLE DATA             |
%           +-------------------------------------------+
%           |------------ Retire Group G_k -------------|------------ Retire Group G_{k+1} ...
%   [Inst 1, Inst 2, ... Inst 100]     [Inst 101, Inst 102, ...]
%                   |
%                   | (Lossy Compression)
%                   V
%               [ IPC_k ]                           [ IPC_{k+1} ]    <- Aggregate "anchor" signal
 
    \label{fig:benchmarks}
\end{figure}
% =====================================================================
% Methodology
% =====================================================================
Traditional supervised learning assumes an instance label per sample. In our setting, supervision is inherently \emph{aggregate}:  throughput metrics (labels) emerges only when groups of instructios are retired from the ROB, so what we observe is a group‑level metric e.g. IPC. Learning from aggregates (a.k.a. learning from label proportions, LLP) replaces instance labels with bag‑level statistics; the learner then finds an instance‑level signal whose \emph{bag means} match the observed labels or label proportions~\cite{yu2014-llp,scott2020-llp,law2018-agg,zhang2020-agg}. Retire groups are our bags, the observaed label is standardized group IPC, and the target is to predict a contiguous per‑instruction surrogate \(s_t\) that is \emph{consistent} with observations:
$$
\forall k:\quad \frac{1}{|\mathcal{G}_k|}\sum_{t\in \mathcal{G}_k} s_t \;=\; \tilde{r}_k.
$$

Mean‑matching alone is under‑determined; temporal structure must resolve the degrees of freedom. We therefore cast the task of surrogate signal prediction as time‑series imputation: a bidirectional Transformer, trained with masked‑sequence objectives, supplies long‑range context to denoise and interpolate, while a smoothness prior (e.g., total‑variation/AR regularization) biases \(s_t\) to be piecewise‑contiguous rather than jagged. This LLP‑imputation pairing leverages exactly the information we have (bag means) and exactly the structure the trace exhibits (local continuity with occasional transitions), yielding a latent that is both aggregate‑correct and friendly for further change point detection (CPD)~\cite{yu2014-llp,scott2020-llp,law2018-agg,zhang2020-agg}.

Once \(s_t\) is generated, we run exact \pelt with autoregressive segment costs to locate change points and keep compact, budget‑matched windows around them, pruning the rest. Because the retained neighborhoods collectively reproduce the full‑trace throughput while excising long stationary spans, IPC error remains low and end‑to‑end runtime shrinks roughly in proportion to the pruned fraction, up to fixed overheads (e.g., warmup). The result composes seamlessly within SimPoint/LoopPoint regions without altering their selections or weights.
 

\section{Background and Related Work}\label{background}

Detailed architectural simulation is indispensable for design space exploration, yet end-to-end cycle accuracy across billions of instructions is prohibitively slow. Prior work to accelerate studies falls into two broad families: profile-driven region selection (\emph{sampling}) and \emph{statistical/synthetic} approaches that model or regenerate execution to shorten runs.  Below we position VitaBeta with respect to each family and to recent learning-based techniques. 

\subsection{Sampling of Representative Regions}
SimPoint introduced basic block vectors (BBVs) and clustering to identify representative simulation points that capture large-scale program behavior~\cite{simpoint-asplos02}. In parallel, SMARTS established statistically rigorous periodic sampling with warmup and confidence bounds for single-threaded applications~\cite{SMARTS1}. For multi-threaded workloads, several methods leverage structure beyond per-thread BBVs. BarrierPoint identifies inter-barrier regions using synchronization (e.g., OpenMP barriers), and selects representative \emph{barrierpoints} using hybrid code/data signatures (BBVs and LRU stack distance vectors)~\cite{barrierpoint}. LoopPoint uses loop-centric profiling to carve repeatable regions, then drives checkpointed sampled simulation in full-system and user modes~\cite{looppoint}. Jiang et al.~\cite{jiang2015_taco} proposed a two-level hybrid method that further refines sampling for multi-threaded workloads. More recent work explores online, adaptive sampling (e.g., Pac-Sim) that learns region representativeness at runtime~\cite{pacsimm-taco24}. 
 
These techniques answer \emph{what} macroscopic regions to simulate. \name is complementary: it prunes \emph{within} selected regions using \pts+\cpd to retain only compact neighborhoods around transitions.
In our experiments, we integrate with both SimPoint and LoopPoint without altering their selection mechanisms.

\subsection{Statistical and Synthetic Simulation}
Statistical simulation replaces long detailed runs with synthetic traces or build models from observed distributions of instruction types, dependencies, and memory access patterns, ~\cite{Nussbaum2009StatisticalSB}. \emph{Statistical Flow Graphs}~\cite{wunderlich2003smarts} model control flow and data dependencies to produce representative workloads. {More recent approaches refine this by capturing more complex behaviors}. \emph{Hierarchical Reuse Distance (HRD)}~\cite{hrd} improves cache miss estimation by capturing locality across multiple granularities. Additionally, \emph{Mocktails}~\cite{mocktails} synthesizes spatial-temporal memory patterns for heterogeneous computing devices, thereby bridging the gap between proprietary IP and academic simulation models.  These approaches can be extremely fast, but they often require careful calibration and may under-model local microarchitectural dynamics (e.g., prefetcher  interactions) that matter to modern memory systems.  

\name differs by never generating synthetic program traces: it keeps real instructions near transitions and prunes stable spans.

\subsection{Microarchitectural Metrics}
Locality analysis (e.g., reuse/stack distance) provides powerful summaries of temporal reuse~\cite{mattson70, locality_theory}. Such metrics underpin a broad literature in cache modeling and can help detect regions with limited sensitivity to cache size or replacement. Related to exploiting redundancy, recent microarchitectural proposals identify \emph{likely-stable loads} and eliminate their execution under safety conditions (e.g., Constable)~\cite{constable}. While such mechanisms reduce work during hardware execution, their direct use as pruning rules in trace-driven simulation is brittle: a single instruction-level heuristic rarely correlates with full pipeline and memory-system responses across diverse contexts. 
% This informs redundancy in existing traces but does not directly drive simulation acceleration across diverse pipelines.
% Our preliminary study (Section~\ref{sec:intuition}) shows that standalone heuristics (stable-loads, reuse-distance and footprint filters) are either accurate but low-gain or higher-gain but error-prone. VitaBeta instead seeks a \emph{phase-sensitive} driver that is contiguous in instruction order and anchored to measured performance statistics.

\subsection{Machine Learning for Computer Architecture}
Machine learning has impacted several architectural problems. Neural and RL-based prefetchers model address sequences or decision policies (e.g., Learning Memory Access Patterns~\cite{hashemi2018learning}, Voyager~\cite{voyager} and production-grade designs such as SPP~\cite{spp-micro16}, Bingo~\cite{bingo-hpca19}, and Berti~\cite{berti-micro22}). Separately, Ithemal learns basic-block throughput directly from instructions, outperforming analytical models in throughput estimation~\cite{mendelson1997speculative}. 
On the other hand, methods like TransFetch~\cite{transfetch} and Pythia~\cite{pythia21} apply attention-based networks and reinforcement learning, respectively, to generate more accurate memory prefetch requests. Recent work~\cite{mine} investigates the use of large language models for memory trace synthesis.
Our training objective for \pts connects to learning-from-aggregates: LLP and related frameworks learn instance-level predictors from bag-level labels or means~\cite{yu2014-llp,scott2020-llp,law2018-agg,zhang2020-agg}. Here, \emph{retire groups} are bags and group throughput is the regression target.
 

% \subsection{Relation to Concurrent Work and Practical Integration} 

In summary, VitaBeta advances trace reduction by (i) introducing an aggregate-consistent, contiguous per-instruction surrogate for microarchitectural responsiveness; (ii) using principled CPD to delineate compact phase neighborhoods; and (iii) operating orthogonally to established region-selection methods to harvest additional speedups without sacrificing fidelity.
% \section{Motivation}

 




% ### paper edit starts
% The whole section of Methodology needs a complete, comprehensive and careful rewritten so that the reviewers from top-tier conferences like ISCA, HPCA and MICRO can understand it. You also need to check for any incomplete logic chains, weak mathematical deduction and faulty mathematical formulations, make sure no problem occurs here. I have also appended a historic draft version of this section at the end. 
\section{Methodology}
\label{sec:methodology}

We operate strictly \emph{within} representative regions chosen by SimPoint/LoopPoint. For each region we (1) learn a contiguous per-instruction \emph{Phase-Transition Signal} (\pts) by matching retire-group averages to standardized anchors (single profiling pass), (2) detect change points (\cpd) on \pts\ with exact \pelt, and (3) preserve compact neighborhoods around these boundaries to match a user-specified reduction budget. Upstream region selection and weights remain unchanged.
% #  \emph{Abbreviations:} FR—feature reconstruction, MI—masked imputation, GAM—group‑average matching, \pts—Phase‑Transition Surrogate, \cpd—change‑point detection.
\subsection{Data, Anchors, and Notation}
Let the dynamic instruction indices in a region be \(T=\{1,\ldots,|T|\}\). Each instruction \(t\) has a feature vector \(u_t\in\mathbb{R}^{D_f}\) (e.g., PC, opcode, address abstractions, locality summaries). The simulator partitions the stream into \emph{retire groups} \(\{\mathcal{G}_k\}_{k=1}^K\) at ROB emission; the mapping \(g(t)=k\) associates instruction \(t\) with its retire group. From a single profiling pass, we observe a \emph{group-level anchor} \(r_k\) (e.g., IPC for group \(k\)). We standardize anchors per stream:
\[
\tilde{r}_k=\frac{r_k-\bar{r}}{\mathrm{sd}(r)},\qquad \bar{r}=\frac{1}{K}\sum_{k=1}^{K}r_k.
\]

\subsection{Phase-Transition Signal (\pts) and Group-Average Matching (\gam)}
A sequence model \(f_\theta\) (Transformer backbone or any strong imputer) emits a scalar \(\pts\) and reconstructed features:
\[
s_t,\,\tilde{u}_t \;=\; f_\theta\!\big(\{u_\tau\}_{\tau\in\mathcal{W}(t)},\,\mathrm{PE}(t)\big),
\]
where \(\mathcal{W}(t)\) is a local or dilated context and \(\mathrm{PE}(t)\) positional encoding. The \emph{group-average matching} constraint enforces aggregate consistency:
\begin{equation}
\mu_k(s)\ \triangleq\ \frac{1}{|\mathcal{G}_k|}\sum_{t\in\mathcal{G}_k}s_t\ \approx\ \tilde{r}_k.
\label{eq:group-avg}
\end{equation}
Intuitively, \(s_t\) is \emph{not} “instruction-level IPC”; it is a contiguous surrogate whose retire-group mean equals the standardized throughput anchor.

Figure~\ref{fig:saits} instantiates the training pipeline on the toy dataset in Table~\ref{tab:example}: instruction features \(u_t\) are partially masked for MI while observed entries drive FR; the imputer predicts reconstructed features \(\tilde{u}_t\) and a per‑instruction surrogate \(s_t\); finally, \gam enforces \(\mu_k(s)=\tilde{r}_k\) within each retire group \(\mathcal{G}_k\) as in \eqref{eq:group-avg}.

\begin{table}[!htbp]
\centering
\small
\caption{Example of profiled dataset from an initial simulation  with group \emph{anchors} for training.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\textbf{Index (\(t\))} & \textbf{PC} & \textbf{Address} & \textbf{Opcode} & \textbf{RD} & \textbf{Group ID} & \textbf{Anchor \(r_{g(t)}\)} \\
\midrule
1 & 0x0018 & 16  & 2 & $\infty$ & 17 & 0.132 \\
2 & 0x0040 & 32  & 0 & 2        & 18 & 0.425 \\
3 & 0x0044 & 8   & 0 & 3        & 19 & 0.345 \\
4 & 0x004C & 64  & 1 & 4        & 19 & 0.345 \\
5 & 0x0050 & 1   & 2 & $\infty$ & 19 & 0.345 \\
\bottomrule
\end{tabular}}
\label{tab:example}
\end{table}


\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.99\linewidth]{images/ort_mit2.pdf}
\caption{End‑to‑end training flow used in \name. Inputs are instruction features \(u_t\) and retire‑group anchors \(r_k\) from a single profiling pass. During training we (i) randomly mask feature subsets (MI) while retaining others (FR), (ii) use a bidirectional Transformer to reconstruct \(\tilde{u}_t\) and predict a contiguous per‑instruction Phase‑Transition Surrogate \(s_t\), and (iii) enforce group‑average matching (GAM) so that the within‑group mean \(\mu_k(s)\) equals the standardized anchor \(\tilde{r}_k\). A total‑variation prior encourages piecewise smooth \(s_t\). At inference, masks are removed and only \(s_t\) is emitted; in the downstream pipeline, \(s_t\) is passed to \cpd\ to locate compact phase boundaries for budgeted pruning. }
\label{fig:saits}
\end{figure*}


\subsection{Transformer Imputer and Weakly Supervised Objective}
We train with masked modeling and aggregate consistency. Concretely, the loss is
\begin{align}
\mathcal{L}_{\mathrm{FR}} &=\frac{\sum_{t,d}M_t^{(d)}\,\ell_d(\tilde{u}_t^{(d)},u_t^{(d)})}{\sum_{t,d}M_t^{(d)}}
\quad\text{(feature reconstruction on observed entries)}, \nonumber\\
\mathcal{L}_{\mathrm{MI}} &=\frac{\sum_{t,d}I_t^{(d)}\,\ell_d(\tilde{u}_t^{(d)},u_t^{(d)})}{\sum_{t,d}I_t^{(d)}}
\quad\text{(masked imputation on artificial masks)}, \nonumber\\
\mathcal{L}_{\mathrm{AC}} &= \frac{1}{|T|}\sum_{k=1}^K |\mathcal{G}_k|\,\rho_\delta\!\big(\mu_k(s)-\tilde{r}_k\big)
\quad\text{(aggregate consistency, Huber)}. \label{eq:ac}
\end{align}
To promote contiguity of \pts\ (for \cpd) and resolve affine ambiguity, we add 
\begin{align}
\mathcal{L}_{\mathrm{TV}} &= \frac{1}{|T|-1}\sum_{t=2}^{|T|} |s_t-s_{t-1}|, \text{(total\ variation)}
% \qquad
% \mathcal{L}_{\mathrm{NORM}} = \big(\bar{s}\big)^2+\big(\Var(s)-1\big)^2, \nonumber
\end{align}
with \(\bar{s}=\tfrac{1}{|T|}\sum_{t=1}^{|T|} s_t\).
The full objective is
\begin{equation}
\mathcal{L} =\mathcal{L}_{\mathrm{FR}} + \lambda\,\mathcal{L}_{\mathrm{MI}} + \gamma\,\mathcal{L}_{\mathrm{AC}} +  \eta\,\mathcal{L}_{\mathrm{TV}}. \label{eq:full}
\end{equation}

\paragraph{Practicalities.}
We train on overlapping tiles (to bound memory), weight retire groups in-batch proportional to counts so that \eqref{eq:ac} approximates the per-instruction normalization, and blend \pts\ in overlaps at inference time. We begin with larger \(\eta\) to avoid jagged \pts, then anneal \(\eta\) so that \pts\ tightens around genuine transitions.

\subsection{Affine Identifiability under Standardized Anchors}
\label{subsec:identifiability}
Assume anchors \(\{\tilde{r}_k\}\) are standardized per stream and training enforces \(\bar{s}\!\approx\!0\) and \(\Var(s)\!\approx\!1\). Under a strictly convex loss near zero in \eqref{eq:ac}, any affine transform \(s'_t=a s_t+b\) that approximately minimizes \(\mathcal{L}_{\mathrm{AC}}\) must satisfy \(a\!\approx\!1\), \(b\!\approx\!0\).
 
\noindent\emph{Sketch.}
Group averaging gives \(\mu_k(s')=a\mu_k(s)+b\). Minimizing \(\sum_k|\mathcal{G}_k|\,\rho_\delta(\mu_k(s')-\tilde{r}_k)\) over \((a,b)\) with the moment constraints yields normal equations whose unique solution is \(a{=}1,b{=}0\) (up to small optimization error) when \(\mathrm{Cov}(\mu(s),\tilde{r})>0\).


\subsection{Change-Point Detection on \pts}
\label{subsec:cpd}
We segment \(s_{1:|T|}\) with \pelt~\cite{pelt12} using an \emph{autoregressive} segment cost of order \(p\) and a BIC-style penalty \(\beta=\alpha\,(p{+}1)\log n\) for tile length \(n\). We sweep a small grid of \(\alpha\) values to obtain candidate boundary sets \(\{\mathcal{C}_j\}\). For each \(\mathcal{C}_j\), we run the budgeted windowing step (next) and select the segmentation whose preserved length is closest to the target (ties broken by fewer segments). This avoids manual penalty tuning while preserving \pelt’s exactness per \(\beta\). For very long traces we apply CPD on overlapping tiles and deduplicate near tile edges.

\subsection{Budgeted Windowing around Detected Boundaries}
Given change points \(\mathcal{C}\subset\{1,\ldots,|T|\}\), we preserve asymmetric windows around each boundary and prune the rest to match target reduction \(r\in(0,1)\). Windows expand or shrink via binary search until the total preserved instructions meet the budget; overlaps are merged.

\begin{algorithm}[t]
\caption{Adaptive Windowing for Budget-Matched Preservation}
\label{alg:adaptive}
\small
\begin{algorithmic}[1]
\Require Change points \(\mathcal{C}\), total length \(|T|\), target reduction \(r\), tolerance \(\epsilon\)
\State \(D\gets \lfloor |T|\,(1-r)\rfloor\); \(W_{\min}\gets 1\); \(W_{\max}\gets |T|\); \(\mathcal{M}^\star\gets\varnothing\); \(P^\star\gets 0\)
\Function{MakeWindow}{$c,W$}
  \State \(\Delta_L\gets\lfloor(W-1)/2\rfloor\), \(\Delta_R\gets W-1-\Delta_L\)
  \State \(s\gets\max(1,c-\Delta_L)\); \(e\gets\min(|T|,c+\Delta_R)\)
  \State \Return \([s,e]\)
\EndFunction
\While{\(W_{\min}\le W_{\max}\)}
  \State \(W\gets\lfloor(W_{\min}+W_{\max})/2\rfloor\)
  \State \(\mathcal{R}\gets \{\textsc{MakeWindow}(c,W):c\in\mathcal{C}\}\); \(\mathcal{M}\gets \textsc{MergeOverlaps}(\mathcal{R})\)
  \State \(P\gets\sum_{[s,e]\in\mathcal{M}}(e-s+1)\)
  \If{\(|P-D|<|P^\star-D|\)} \(\mathcal{M}^\star\gets\mathcal{M};\ P^\star\gets P\) \EndIf
  \If{\(|P-D|\le D\epsilon\)} \Return \(\mathcal{M}\) \EndIf
  \If{\(P < D\)} \State \(W_{\min}\gets W+1\) \Else \State \(W_{\max}\gets W-1\) \EndIf
\EndWhile
\State \Return \(\mathcal{M}^\star\) \Comment{Best-effort if tolerance unmet}
\end{algorithmic}
\end{algorithm}

\subsection{A Learning-Free Surrogate (Beta Metric)}
\label{subsec:beta}
When learning is unavailable, we use a heuristic surrogate that is contiguous and phase-sensitive:
\begin{equation}
\label{eq:beta}
B_t \;=\; \alpha_1\,\ln\big(1+\mathrm{RD}_t\big)\;+\;\alpha_2\,\big|\Delta\mathrm{PC}_t\big|\;+\;\alpha_3\,\big|\Delta r_{g(t)}\big|,
\end{equation}
where \(\mathrm{RD}_t\) is reuse distance, \(\Delta\mathrm{PC}_t\) encodes control-flow change, and \(\Delta r_k=r_k-r_{k-1}\) is the adjacent \emph{group}-anchor gradient, broadcast to instructions in group \(k\). We use \((\alpha_1,\alpha_2,\alpha_3)=(0.5,0.3,0.2)\). CPD and Algorithm~\ref{alg:adaptive} apply unchanged to \(B_t\).

\subsection{Sanity Bounds for Pruning Error}
Suppose CPD partitions \(T\) into segments \(\{\mathcal{S}_j\}\) within which a length-normalized additive performance functional \(F(t)\) varies by at most \(\Delta_j\). If all true regime boundaries lie inside preserved windows and we remove \(N_{\mathrm{pruned}}\) instructions across segments, then
\[
\big|\widehat{F}-F\big|\ \le\ \frac{1}{|T|}\sum_j \Delta_j\, |\mathcal{S}_j\setminus\text{kept}|\ \le\ \Delta_{\max}\frac{N_{\mathrm{pruned}}}{|T|},
\]
so for piecewise-stable behaviors the error scales with the pruned fraction, while preserving transition neighborhoods controls bias.

\subsection{Complexity and Deployment}
Learning uses tiled Transformer inference and batched \gam; \pelt\ is near-linear for fixed penalty families~\cite{pelt12,ruptures20}. The pipeline runs a single profiling pass to extract anchors, trains once per region type or workload family (or uses the learning-free surrogate), and emits keep-lists that downstream simulators apply within existing SPs/RIDs without altering their selection or weights.

  
 

\section{Evaluation on Single-Core Applications} 
\label{evaluation_sc}
We evaluate VitaBeta on reduced SimPoint traces versus full SimPoint traces.

\subsection{Simulation Methodology}
\label{subsec:expsetup}
\textbf{Workloads.} We select memory-intensive traces from SPEC CPU2006~\cite{spec2006}, SPEC CPU2017~\cite{spec2017}, GAP~\cite{beamer2017gapbenchmarksuite}, and ten server workloads collected with gem5~\cite{gem52011} in full-system mode and converted to ChampSim format~\cite{llbp_workloads}. We use reference inputs for SPEC and real/synthetic inputs for GAP.

The ten server traces consist of real-world applications: Eight traces from Java benchmark suites: BenchBase \cite{oltp_bench}, Renaissance \cite{java_renaissance}, and DaCapo \cite{java_dacapo}. Two traces capturing web-server activity: Node.js and PHP-FPM.
% \end{itemize}

\textbf{Environment.} Simulations use a modified ChampSim\cite{ChampSim}, coupled with Ramulator 2.0\cite{luo2023ramulator20modernmodular} for DRAM modeling. We warm up for 10M instructions and collect stats for the next 500M.

\textbf{System configuration.} Table~\ref{tab:baseline} lists a modern OoO baseline. Aggregate anchor signals including IPC (training data) are collected in a fast pass without prefetchers (about 29.27\% faster than prefetcher-enabled simulation), then used to train/apply the PTS model.

\begin{table}[!htbp]
\centering 
\caption{Simulation parameters of the baseline system.}
\begin{tabular}{|l|l|}
\hline
\textbf{Core} & \begin{tabular}[c]{@{}l@{}}
Out-of-order, 4\,GHz \\
8-fetch/decode, 6-dispatch/retire, 512-entry ROB\\
192-entry LQ, 114-entry SQ, 205-entry scheduler
\end{tabular} \\
\hline
\textbf{Caches} & \begin{tabular}[c]{@{}l@{}}
L1I: 32 KB, 64 sets, 8-way, 64B lines, 3 cycles \\
L1D: 48 KB, 64 sets, 12-way, 64B lines, 4 cycles \\
L2: 512 KB, 1024 sets, 8-way, 64B lines, 9 cycles \\
LLC: 2 MB, 2048 sets, 16-way, 64B lines, 40 cycles, LRU 
\end{tabular} \\
\hline
\textbf{BP} & Hashed perceptron predictor~\cite{hashed_perceptron} \\
\hline
\textbf{DRAM} & \begin{tabular}[c]{@{}l@{}}
DDR5-3200, Ramulator 2.0 \\
2 ch, 2 rank/ch, 8 bank groups\\
4 banks/group, 32-bit channel \\
tCAS/tRCD/tRP: 24/24/24, tRAS: 52 \\
Zen4-like mapping
\end{tabular} \\
\hline
\end{tabular}
\label{tab:baseline} 
\end{table}

\textbf{Prefetchers.} We test BERTI~\cite{navarro2022berti}, Bingo~\cite{bingo}, and SPP~\cite{spp}.

\textbf{Model evaluation.} 
We consider:
(i) \emph{pre-trained} PTS models trained on a single representative trace and applied across benchmarks, and
(ii) \emph{application-specific} models trained on the top-weight SimPoints of each application. Unless stated otherwise, results use an ImputeFormer pre-trained on a 10B-instruction \texttt{bfs-10} trace.
We retain SimPoint weights unchanged. For each SimPoint, VitaBeta outputs a reduced trace by concatenating preserved chunks; the final result is the weighted sum over SimPoints.

\subsection{Overhead Analysis}
\label{subsec:overhead}
Let $T_{\text{full}}$ be the time to simulate a full trace once, $T_{\text{prof}}$ the one-time simulation pass used to extract features, and $O$ the remaining one-time overheads (model inference or training, plus CPD+pruning). With reduction factor $r$, each pruned simulation takes $T_{\text{full}}/r$, so the per-run saving versus full simulation is $T_{\text{full}}\!\left(1-\frac{1}{r}\right)$. The break-even number of reduced runs that amortizes the one-time costs is
\begin{equation}
\label{eq:breakeven}
n_{\text{break}} \;\ge\; \frac{r\,(T_{\text{prof}} + O)}{T_{\text{full}}(r-1)}.
\end{equation}
If the profiling pass can be amortized or reused across a design sweep, set $T_{\text{prof}}\!\approx\!0$; otherwise a conservative choice is $T_{\text{prof}}\!\approx\!T_{\text{full}}$. In our setup, typical $T_{\text{full}}$ ranges from $\sim$2\,h (603.bwaves\_s-2609B) to $\sim$6\,h (605.mcf\_s-1644B). 

The one-time overhead beyond profiling is $O{=}0.5$\,h for inference with a pre-trained model or $O{\approx}36$\,h for training from scratch, plus $53$\,min for CPD+pruning (measured on NVIDIA RTX~4090 for learning and Intel Xeon Gold~6338 for simulation/pruning), yielding $O{=}1.38$\,h (pre-trained) or $O{=}36.88$\,h (custom training). As a concrete example, with $r{=}8$ and $T_{\text{full}}{=}5$\,h (bc-12), taking $T_{\text{prof}}{=}T_{\text{full}}$ gives
$n_{\text{break}} \!\ge\! \tfrac{8(5+1.38)}{5\cdot7}\!\approx\!1.46$ (two runs) for the pre-trained case, and
$n_{\text{break}} \!\ge\! \tfrac{8(5+36.88)}{5\cdot7}\!\approx\!9.57$ (about ten runs) when training from scratch. Hence, a pre-trained model yields net benefit after only a few reduced simulations, whereas custom training pays off when many runs are planned.


\subsection{Metrics}
We report the following performance metrics:
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{IPC Error ($E_{\text{IPC}}$)}: The relative error in cumulative IPC between the reduced and full-length trace simulations.
    \item \textbf{Cache Miss Rate Error ($E_{\text{Cache\_miss}}$)}: The relative error in cache miss rates, averaged across all cache levels.
    \item \textbf{Cache Access Latency Error ($E_{\text{Cache\_latency}}$)}: The relative error in average cache access {latency (lat.)}.
    \item \textbf{TLB Miss Rate Error ($E_{\text{TLB\_miss}}$)}: The relative error in miss rates for Translation Lookaside Buffer.
    \item \textbf{TLB Latency Error ($E_{\text{TLB\_latency}}$):} The relative error in averaged TLB access {latency (lat.)}.
    \item \textbf{Simulation Speed-up}: The factor by which the simulation time is reduced compared to the original SimPoint trace.
\end{itemize}
For prefetchers we also report the error of relative IPC improvement, and errors of prefetch accuracy and coverage.
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{IPC Improvement}: The proportional improvement in IPC when the prefetcher is enabled, relative to a no-prefetcher baseline:
    \[
    \text{IPC Improvement} = \frac{\text{IPC}_{\text{with prefetcher}} - \text{IPC}_{\text{no prefetcher}}}{\text{IPC}_{\text{no prefetcher}}}.
    \]
    \item \textbf{Prefetch Accuracy}: The ratio of effective (USEFUL) prefetches to the total number of issued prefetches:
    \[
    \text{Accuracy} = \frac{\text{USEFUL}}{\text{USEFUL} + \text{USELESS}}.
    \]
    \item \textbf{Prefetch Coverage}: The fraction of cache-miss events that are mitigated by effective prefetching, defined as :
    \[
    \text{Coverage} = \frac{\text{USEFUL}}{\text{USEFUL} + \text{LOAD\_MISS} + \text{RFO\_MISS}}.
    \]
\end{itemize}
\begin{figure*}[!htbp] 
    \centering
    \subfloat[Cumulative IPC error (\%).]{
        \includegraphics[width=0.325\textwidth]{individual_boxplot_RDR_IPC_mean_error.pdf}
    }
    \subfloat[Cache miss error (\%).]{
        \includegraphics[width=0.325\textwidth]{individual_boxplot_RDR_cache_miss_error.pdf}
    }
    \subfloat[Cache latency error (\%).]{
        \includegraphics[width=0.325\textwidth]{individual_boxplot_RDR_cache_latency_error.pdf}
    }
    \caption{Error distributions for $2\times$, $4\times$, and $10\times$ reductions on SPEC2006/2017, GAP, and server workloads using a pre-trained PTS model. Average speedups: $1.9\times$, $3.9\times$, $9.2\times$. \textit{\small Note: Unless stated otherwise, results in the section use an ImputeFormer pre-trained on a 10B-instruction \texttt{bfs-10} trace.}}
    \label{fig:distribution_perf}
\end{figure*}


\subsection{Results}
\subsubsection*{General Performance}
Figure~\ref{fig:distribution_perf} shows that about 75\% of traces have IPC error below 5\% at $2\times$ reduction; this increases modestly at $10\times$ while speedups scale sublinearly with the reduction factor due to fixed costs. Outliers are bandwidth-bound kernels (e.g., \texttt{GemsFDTD}, \texttt{bwaves}, \texttt{roms}, \texttt{mcf}) where small misalignment in preserved segments can amplify cache errors; even then, IPC errors generally remain below 17.5\%.

Overall, the boxplots confirm that VitaBeta maintains traces of their architectural fidelity across diverse application domains, with only a handful of predictable outliers, and that performance gains scale almost proportionally with trace-length reduction.
\begin{figure}[!htbp]
    \centering
    \setlength{\belowcaptionskip}{-10pt}
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \includegraphics[width=0.99\linewidth]{images/overall_performance/error_metrics_speedup_benchmark_comparison.pdf}
    \caption{Benchmark-level performance with a pre-trained PTS model at $2\times$ reduction. Error bars: standard deviation.}
    \label{fig:benchmarks}
\end{figure}

\subsubsection*{Across Benchmark Suites}
Figure~\ref{fig:benchmarks} shows SPEC2017 attains the lowest cache errors; GAP exhibits higher IPC and TLB-related errors due to irregular memory behavior and large footprints, yet still benefits from reduction.

\begin{figure*}[!htbp]
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_trace_reduction_rate.pdf}
        \caption{Relative IPC improvement error (\%).}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_on_accuracy_vs_trace_reduction_rate.pdf}
        \caption{Prefetch accuracy error (\%).}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_on_coverage_vs_trace_reduction_rate.pdf}
        \caption{Prefetch coverage error (\%).}
    \end{subfigure} 
    \caption{Relative prefetcher accuracy on pruned traces (pre-trained PTS model), averaged over SPEC2006/2017, GAP, and server workloads. Mean IPC improvements for full-traces: BERTI 20.28\%, Bingo 27.80\%, SPP 34.50\%.}
    \label{fig:prefetch_perf}
\end{figure*}

\begin{figure*}[!htbp] 
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_benchmark_2X_rdr.pdf}
        \caption{IPC improvement error at $2\times$.}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_benchmark_4X_rdr.pdf}
        \caption{IPC improvement error at $4\times$.}
    \end{subfigure}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/prefetch/rel_abs_difference_vs_benchmark_10X_rdr.pdf}
        \caption{IPC improvement error at $10\times$.}
    \end{subfigure} 
    \caption{Prefetcher IPC-improvement errors across benchmark suites (pre-trained PTS model).}
    \label{fig:prefetch_benchmark}
\end{figure*}

\subsubsection*{Relative Prefetcher Performance}
Figure~\ref{fig:prefetch_perf} shows that the ordering among BERTI, Bingo, and SPP is preserved up to $4\times$ reduction and remains informative at $10\times$ for workloads with larger baseline gains (e.g., GAP). 
% Prefetch accuracy and coverage errors stay below $\sim$1–1.5\%.

 At higher reduction rates from (\(8\times\) to \(10\times\)), however, the error can exceed half the difference between the IPC improvements of any two prefetchers and the results may not be reliable. This effect is particularly pronounced for benchmarks with low baseline IPC improvements. For example, Figure~\ref{fig:prefetch_benchmark} shows that server traces, with a baseline improvement of only 7.64$\%$, exhibit relative errors of 1.41\%, 3.17\%, and 3.95\% under \(2\times\), \(4\times\), and \(10\times\) reductions, respectively. In contrast, benchmarks with higher baseline IPC improvements (e.g., GAP with 63.32\%) maintain a robust relative accuracy of prefetchers even at higher reduction rates.
 
Additionally, prefetch accuracy and coverage errors stay below $\sim$1–1.5\% for trace reductions between \(2\times\) and \(10\times\). These metrics are retained very similar   between both sets of runs and help prefetcher designer make the right analysis and conclusions
\begin{figure*}[!htbp]
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{server_2x_performance_comparison.pdf}
        \caption{$2\times$ reduction.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{server_4x_performance_comparison.pdf}
        \caption{$4\times$ reduction.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{server_10x_performance_comparison.pdf}
        \caption{$10\times$ reduction.}
    \end{subfigure}
 \caption{Performance on server workloads with various prefetchers: metric errors vs. reduction (pre-trained PTS model).}
    \label{fig:abs_prefetch_perf}
\end{figure*}

\subsubsection*{Server Workloads}
Figure~\ref{fig:abs_prefetch_perf}  summarizes the performance of VitaBeta with 3 different prefetchers  on 10 server workloads. It shows most metrics remain within $\le$10\% error up to $10\times$ reduction, with speedups saturating near $9.4\times$ due to fixed costs. TLB miss errors are higher than SPEC/GAP at aggressive reductions, reflecting large, locality-optimized heaps and GC behavior that make TLB dynamics sensitive to trace pruning.

\subsubsection*{Branch Prediction Accuracy}
Because VitaBeta preserves sizable chunks around change points, branch distributions remain close to the originals. At $2\times$ reduction with a pre-trained model, BP accuracy errors remain small (e.g., 0.126\% SPEC2006, 0.117\% SPEC2017, 0.489\% GAP) despite significant instruction pruning in GAP.

\begin{figure*}[!htbp]  
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \includegraphics[width=.99\textwidth]{publication_subplots_hatched_bars.pdf}
    \caption{Different Backbone Models for generating PTS: Beta heuristics, non-Transformers, and Transformer variants at $4\times$ reduction (no prefetchers enabled). Error bars: standard deviation.}
    \label{fig:model_perf}
\end{figure*}




 
\subsection{Sensitivity Analysis}
\subsubsection*{Model Choices}
Figure~\ref{fig:model_perf} compares Various PTS backbones. Transformer variants (SAITS/ImputeFormer) outperform non-Transformers (TEFN~\cite{TEFN}, ModernTCN~\cite{moderntcn}) and the heuristic Beta Metric, especially at higher reductions. Application-specific training offers up to $\sim$10\% additional error reduction over a strong pre-trained model; heavier models (TimesNet, TimeMixer) do not consistently outperform compact Transformers given similar training budgets.

\subsubsection*{Impact of Training Data}
Table~\ref{tab:sensitivity_training_onecol} indicates that pre-training on different representative traces yields similar aggregate accuracy; \texttt{gcc-13B} pre-training slightly improves means but increases variance due to its chaotic phase structure~\cite{Shen+:ASPLOS04}. % CITATION NEEDED


\begin{table}[htbp!]
  \caption{Different Pre-Training Traces (SPEC2017/2006/GAP), $2\times$ reduction. Prefetcher: BERTI. Values are geometric means in \%. Lower is better except Speedup.}
  \label{tab:sensitivity_training_onecol}
  \centering
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{max width=1.0\columnwidth}
  \begin{tabular}{l|rrrrrr}
  \toprule
  Training data & ${E}_{\text{IPC}}$  & $E_{\text{cache\_miss}}$  &$E_{\text{cache\_latency}}$  & $E_{\text{TLB\_miss}}$    & $E_{\text{TLB\_latency}}$   & Speedup \\
  \midrule
  astar-313B & \textbf{3.224\%} & 2.625\% & \textbf{1.783\%} & 3.655\% & 4.213\% & $1.971\times$ \\
  bfs-10     & 3.458\% & \textbf{2.373\%} & 1.926\% & 3.283\% & 4.498\% & $1.952\times$ \\
  gcc-13B    & 3.229\% & 2.456\% & 2.094\% & \textbf{2.787\%} & \textbf{4.190\%} & \textbf{$1.989\times$} \\
  \bottomrule
  \end{tabular}
  \end{adjustbox}
  \vspace{0.25ex}
  \\
  \footnotesize Note: Same ImputeFormer hyperparameters; models are pre-trained on different traces and evaluated across the suites.
\end{table}


\subsubsection*{CPD Cost Functions}
We conduct experiments comparing the {Auto-Regressive ($AR$)} cost function with the {Least Squared Deviation ($L_2$)} cost function. We assess their effectiveness on \textit{TimeMixer} and \textit{TimesNet}, since these two models tend to yield very high variance among all error metrics compared to other Transformer-based models. The $L_2$ cost function is defined as: $$c_{\text{$L_2$}}(y_{t_a:t_b}) = \sum_{t = t_a}^{t_b} \left( y_t - \bar{y}_{t_a:t_b} \right)^2,$$
where $\bar{y}_{t_a:t_b}$ is the mean of the segment $[t_a, t_b]$. 
While $AR$ is suitable for time series where current values are influenced by past values, the $L_2$ cost function is effective for detecting shifts in the mean level of the signal\cite{Lavielle2005}.   

Table~\ref{tab:sensitivity_cpd_onecol} shows TimeMixer prefers AR cost (temporal dependence), while TimesNet prefers $L_2$ (mean shifts). Selecting a CPD cost aligned with the backbone’s inductive bias improves stability.
\begin{table}[htbp!]
  \caption{CPD cost functions with different backbones (SPEC2017/2006/GAP), $2\times$ reduction. Prefetcher: BERTI. Values are geometric means in \%. Lower is better except Speedup.}
  \label{tab:sensitivity_cpd_onecol}
  \centering
  \setlength{\tabcolsep}{2pt}
  \begin{adjustbox}{max width=1.0\columnwidth}
  \begin{tabular}{l|rrrrrr}
  \toprule
  Approach &  $E_{\text{IPC}}$  & $E_{\text{cache\_miss}}$  &$E_{\text{cache\_latency}}$  & $E_{\text{TLB\_miss}}$    & $E_{\text{TLB\_latency}}$   & Speedup\\
  \midrule
  \footnotesize TimeMixer + AR-CPD    & \textbf{2.175\%} & \textbf{1.249\%} & \textbf{1.400\%} & \textbf{2.702\%} & \textbf{3.207\%} & $1.959\times$ \\
  \footnotesize TimeMixer + $L_2$-CPD  & 2.870\% & 1.392\% & 1.419\% & 3.313\% & 4.234\% & \textbf{$2.035\times$} \\
  \midrule
  \footnotesize TimesNet + AR-CPD     & 3.514\% & 1.924\% & 2.675\% & 3.217\% & 4.223\% & $1.995\times$ \\
   \footnotesize TimesNet + $L_2$-CPD     & \textbf{3.027\%} & \textbf{1.789\%} & \textbf{2.356\%} & \textbf{3.495\%} & \textbf{3.720\%} & $1.987\times$ \\
  \bottomrule
  \end{tabular}
  \end{adjustbox}
  \vspace{0.25ex}
  \\
  \footnotesize Note: Each Backbone model share the same hyperparameters; only the CPD cost differs.
\end{table}

\section{Evaluation on Multi-Core Applications} 
\label{evaluation_mc}
\textbf{Experimental setup.} We follow LoopPoint in gem5 full-system~\cite{LoopPointTutorialHPCA23} on seven NAS Parallel Benchmarks~\cite{npb_benchmark} (Class A~\cite{npb_input_size}, 4 OpenMP threads) with a Skylake-like quad-core (Table~\ref{tab:detailed_board}) baseline configuration.

\begin{table}[!htbp]
\centering
\caption{Configuration of detailed gem5 FS simulation.}
\begin{tabular}{|l|l|}
\hline
\textbf{CPU Model} & Out-of-order, 4\,GHz, SkyLakeCPU (4 cores) \\ 
\hline
\textbf{Caches} & \begin{tabular}[c]{@{}l@{}}
L1I/L1D: 64 KB, 8-way;\; L2: 2 MB, 64-way \\
Coherence: MESI two-level
\end{tabular} \\
\hline
\textbf{System} & Ubuntu 24.04 (4 CPUs, built with QEMU~\cite{qemu}) \\
\hline
\textbf{DRAM} & DDR4\_2400\_16x4, 3GB \\
\hline
\end{tabular}
\label{tab:detailed_board}
\end{table}
\vspace{-0.5cm}
\subsection{LoopPoint Workflow}
\begin{enumerate}[nosep, leftmargin=*]
  \item \textbf{Boot Checkpointing.}  
        We fast-forward a Ubuntu 24.04 image to the shell prompt with a
        \textit{X86KvmCPU} and save an \emph{after-boot} snapshot.
        This removes the OS boot path from all subsequent experiments.
  \item \textbf{Process-map Extraction.}  
        Restoring the boot image we launch the
        NPB binary and dump
        \emph{/proc/\$PID/maps}.  
        We filter the map to get (i) the application
        text segment---used later to validate marker PCs—and
        (ii) ``unsafe'' ranges (OpenMP, {pthread} etc.) that must
        be \emph{excluded} from BBV collection. 
  \item \textbf{LoopPoint analysis.}  
         We enable a \emph{LooppointAnalysis} probe that listens       inside the application’s text range; library addresses found in
        the previous step are masked out.  
        Region length  is fixed to 400M instructions.  
        We record the BBV, global instruction count and loop
        counters and then reset all statistics. 
        A typical NPB-A
        run produces $50$–$90$ regions, which are later clustered using k-means for representative  selection.         For every representative region ID (RID) we derive three
        markers (warm-up, start, end).
 
  \item \textbf{Checkpoint construction.}  
        Starting from the boot snapshot,
        we enable a
        \textit{PcCountTracker} that listens at
        \textit{WORKBEGIN}, and dumps a checkpoint whenever a warm-up or
        start marker fires.  
        Regions whose warm-up coincides with
        \textit{WORKBEGIN} are captured automatically, ensuring \emph{one
        micro-checkpoint (LoopPoint) per RID} and zero redundant state.
  \item \textbf{Detailed replay.}  
        We restores each
        LoopPoint on the detailed core
        (Tab.~\ref{tab:detailed_board}).  
        Per-region stats are scaled by the cluster weight and summed;
        the grand total is compared against an unsampled detailed run.
\end{enumerate}

\subsection{Using VitaBeta  for {LoopPoint Refinement}}
\begin{enumerate}[nosep, leftmargin=*]
\item \textbf{Generate elastic traces.}  
For each application profiled by LoopPoint, we load information from the same RID and use a modified \textit{TraceCPU} to  emit an \textit{interleaved trace} similar to Table~\ref{tab:example} from gem5 elastic traces\cite{elastic_trace_gem5} (memory data trace and instruction fetch trace).
\item \textbf{Offline pruning to generate new checkpoints.}  
    We then run VitaBeta with a target $4\times$ reduction to retains only the
      instruction spans around its detected change points. This results in a compact \emph{keep-list}:
      \(\langle\text{PC},\,\Delta\text{inst}\rangle\) pairs that delimit the surviving slices. Finally, we re-checkpoint in the same way as LoopPoint with the generated \textit{PcCountTracker}. Detailed replay on these checkpoints uses the same core configuration (Table~\ref{tab:detailed_board}).
\end{enumerate} 


\begin{figure*}[!htbp] 
    \centering
    \includegraphics[width=1.00\textwidth]{lp2.pdf}
    \caption{Per-benchmark breakdown for LoopPoint+VitaBeta (pre-trained PTS).}
    \label{fig:lp_vb_breakdown} 
\end{figure*}

\begin{figure}[!htbp] 
    \centering
    \includegraphics[width=.99\columnwidth]{lp1.pdf}
    \caption{LoopPoint vs.\ LoopPoint+VitaBeta (against unsampled detailed ground truth).}
    \label{fig:lp_vb_comparison} 
\end{figure}
% \vspace{-0.5cm}

\subsection{Results}
We evaluate three configurations:
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{Ground-truth (GT): }  full, unsampled detailed run.
    \item \textbf{LoopPoint (LP): }  vanilla LoopPoint, average 30 regions.
    \item \textbf{LP+VB: } VitaBeta-refined LoopPoints ($4\times$ reduction).
\end{itemize}
\smallskip\noindent
We sum per-region statistics weighted by region
multiplicity and report the GeoMean error of
\textsc{LP}/\textsc{LP+VB} versus \textsc{GT}.  


Figure~\ref{fig:lp_vb_comparison} shows that \textsc{LP+VB} multiplies
LoopPoint’s original $4.3\times$ speed-up to {23.9$\times$}—a $5.6\times$ additional acceleration,while IPC error rises modestly from $3.1\%$ to {9.6\%}.  Cache and DRAM latency errors remain below $10.5\%$ and $17.0\%$,
respectively.  Although VitaBeta’s pruning is uniformly aggressive, accuracy deteriorates sub-linearly corresponding to the gain in speed-up. Figure~\ref{fig:lp_vb_breakdown} indicates compute-bound workloads
(\textit{CG}, \textit{EP}) gain the most ($42$–$285\times$ speed-up),
whereas memory-intensive ones with irregular spatial locality (\textit{MG}, \textit{LU}) incur 
highest DRAM-latency error.  These high errors are caused by interleaving the multi-threading traces that coincides with page-table walks and conflict-miss bursts. Nevertheless, even for them, the IPC error stays below 20\%, and the simulator still runs an order of magnitude faster than  LoopPoint baseline. 
These results validate VitaBeta as an \emph{orthogonal}
complement to region-level sampling in multi-core full-system studies.  

% \textbf{Discussion.}
Multi-threaded traces introduce cross-core interference and barrier-aligned patterns. Two practical refinements further reduce error: (i) \emph{co-run PTS}: augment features with shared-LLC MPKI and per-core QPS to align cross-core phases; (ii) \emph{sync-guarded pruning}: inflate preservation windows around barrier and lock/unlock PCs. We applied conservative sync guards in our implementation; richer co-run PTS is left for future work.

\section{Limitations and Threats to Validity}
\label{sec:limitations}
% ### paper edit starts
% Please provide a rewritten of the limitation after a carefuly, thorough reading of the original manuscript, and deep thinking, the current one is not informative enough for top-tier conferences like ISCA, HPCA and MICRO 
\textbf{Scope of the surrogate.} PTS is a learned surrogate constrained by group-level anchors; if anchors are noisy or biased by the profiling setup (e.g., prefetchers disabled), PTS may inherit these biases. 

\textbf{Generalization.} Pre-trained models generalize across evaluated suites and three prefetchers, but cross-\emph{microarchitecture} generalization is not guaranteed; re-profiling and light fine-tuning may be required when pipeline widths, cache hierarchies, or memory systems differ substantially. 

\textbf{Warmup and fixed costs.} Speedups saturate below the nominal reduction due to fixed simulation overheads and warmup; aggressive pruning that shortens preserved regions can increase sensitivity to warmup choices.

\textbf{CPD hyperparameters.} CPD quality depends on penalty choices and windowing; misconfigured penalties can under- or over-segment. We use a BIC-style penalty and validated defaults empirically.

\textbf{Multicore synchronization.} For multi-threaded workloads, barriers and lock-intensive regions are sensitive to pruning. We mitigate this with sync guards; more principled co-run PTS features are left for future work.

% ### paper edit ends
\section{Conclusion} \label{summary}
We presented \name, a within‑region trace‑reduction framework that learns a contiguous per‑instruction \pts anchored by retire‑group throughput and uses principled \cpd\ with budgeted windowing to preserve only compact, phase‑defining neighborhoods. This aggregate‑consistent surrogate avoids ill‑posed instruction‑level “IPC” targets while remaining faithful to the measurable statistics that modern OoO cores expose, and composes cleanly with SimPoint and LoopPoint without altering their region selections or weights. A learning‑free fallback (\S\ref{subsec:beta}) enables deployment even when training is infeasible.

Comprehensive evaluation on SPEC CPU2006/2017, GAP, and ten server workloads shows that \name achieves up to $10{\times}$ trace reduction with mean errors of 5.47\% (IPC), 4.01\% (cache miss), and 2.97\% (cache latency), and delivers a $9.23{\times}$ end‑to‑end speedup with a pre‑trained Transformer. Importantly, \name preserves comparative conclusions: across BERTI, Bingo, and SPP, the absolute error in \emph{relative} IPC improvement remains low (4.05\% at $4{\times}$, 5.79\% at $10{\times}$), maintaining prefetcher ranking. On multi‑core full‑system runs, \name multiplies LoopPoint’s speedup by $5.6{\times}$ with moderate accuracy loss (IPC/cache‑miss/cache‑latency errors of 9.6\%/16.3\%/8.3\%), validating orthogonality to region‑level sampling.

By casting fine‑grained pruning as learning from aggregates and coupling it with exact segmentation, \name turns a single profiling pass into a robust driver for high‑fidelity acceleration. Looking ahead, we see three promising directions: (i) cross‑microarchitecture transfer via light re‑profiling and fine‑tuning of \pts; (ii) co‑run \pts features and synchronization‑aware guards to further stabilize multi‑core pruning; and (iii) anchors beyond IPC (e.g., memory‑system signals) to target specific studies. We believe \name provides a practical path to substantially faster, faithful architectural simulation, enabling broader design sweeps and more timely evaluation of emerging microarchitectural ideas.
% \balance
\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}

```tex


original methodology draft (mathematical formulation and other reasoning/deduction):

```tex



\section{Methodology}
\label{sec:methodology}



\subsection{Design Goals and Problem View}
VitaBeta performs fine-grained pruning by learning a contiguous, per-instruction latent signal---the \emph{Phase-Transition Surrogates} (PTS)---subject to aggregate constraints. We \emph{anchor} PTS to retire-group quantities exposed by the simulator (e.g., instruction-per-cycle recorded at ROB retirement). PTS is a context-aware signal designed to be (i) consistent with group-level IPC and (ii) piecewise smooth so that change-point detection (CPD) on PTS aligns with microarchitectural phase shifts. 
% After CPD, we preserve compact neighborhoods around the change points, meeting a target reduction budget.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{images/workflow.pdf}
    \caption{VitaBeta as multivariate imputation with aggregate constraints: features \(\to\) PTS; CPD on PTS \(\to\) budget-matched pruning via Algorithm~\ref{alg:adaptive_window}.}
    \label{fig:workflow}
\end{figure}



% We call the per–retire-group performance quantity logged by the simulator (e.g., the number of instructions retired in a cycle, standardized within a training stream) the \emph{anchor} $r_k$ for group $k$; instructions that retire together constitute a retire group $\mathcal{G}_k$.
\textbf{Clarifications.}
We use \emph{anchor} to denote a measured performance quantity at ROB retire-group granularity (\eg group IPC). Instructions that emit together from ROB buffer form a \emph{retire group}; the anchor value $r_k$ is logged from an initial simulation per group $k$. During training, \pts is constrained so that the \emph{mean} \pts across instructions in group $k$ equals (up to standardization) the anchor $r_k$. 



Figure~\ref{fig:saits} provides a concrete example illustrating the training process using the data from Table~\ref{tab:example}.
\begin{table}[!htbp]
\centering
\small
\caption{Example of profiled dataset from an initial simulation  with group \emph{anchors} for training.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
\toprule
\textbf{Index (\(t\))} & \textbf{PC} & \textbf{Address} & \textbf{Opcode} & \textbf{RD} & \textbf{Group ID} & \textbf{Anchor \(r_{g(t)}\)} \\
\midrule
1 & 0x0018 & 16  & 2 & $\infty$ & 17 & 0.132 \\
2 & 0x0040 & 32  & 0 & 2        & 18 & 0.425 \\
3 & 0x0044 & 8   & 0 & 3        & 19 & 0.345 \\
4 & 0x004C & 64  & 1 & 4        & 19 & 0.345 \\
5 & 0x0050 & 1   & 2 & $\infty$ & 19 & 0.345 \\
\bottomrule
\end{tabular}}
\label{tab:example}
\end{table}


\subsection{Data Model and Anchors}
Let the dynamic instruction sequence be \(T=\{1,\ldots,|T|\}\). Each instruction \(t\in T\) has a feature vector \(u_t\in\mathbb{R}^{D_f}\) (PC, opcode, address abstractions, locality summaries, etc.). The trace is partitioned into retire groups \(\{\mathcal{G}_k\}_{k=1}^{K}\), and we observe an anchor \(r_k\) per group (e.g., group throughput). Let \(g(t)=k\) map instruction \(t\) to its group.

A sequence model \(f_\theta\) (Transformer backbone) outputs PTS
\[
s_t \;=\; f_\theta\!\Big(\{u_\tau\}_{\tau\in\mathcal{W}(t)},\,\mathrm{PE}(t)\Big),
\]
with a local context window \(\mathcal{W}(t)\) and positional encodings \(\mathrm{PE}(\cdot)\). We require \emph{aggregate consistency}:
\begin{equation}
\label{eq:agg-consistency}
\mu_k(s)\;\triangleq\;\frac{1}{|\mathcal{G}_k|}\sum_{t\in\mathcal{G}_k} s_t \;\approx\; \tilde{r}_k,
\end{equation}
where anchors are standardized per training stream,
\[
\tilde{r}_k=\frac{r_k-\bar{r}}{\mathrm{sd}(r)},\quad \bar{r}=\frac{1}{K}\sum_{k=1}^K r_k.
\]

\textbf{Standardization and identifiability.}
Because \eqref{eq:agg-consistency} is affine-invariant in $s$, we remove shift/scale ambiguity by standardizing anchors per stream and penalizing per-chunk PTS moments (zero mean, unit variance), which makes the anchor consistency \emph{identifiable} up to small optimization error. A one-parameter post-hoc scale \(c\) can be retained for diagnostics but is fixed to \(1\) during training.

\subsection{Learning as Imputation with Aggregate Constraints}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.99\linewidth]{images/ort_mit2.pdf}
\caption{Training schematic: feature masking (FR/MI) for PTS prediction.  }
\label{fig:saits}
\end{figure*}



\label{subsec:learning}
We train \(f_\theta\) to (i) reconstruct visible features, (ii) impute artificially masked features, and (iii) produce a PTS that satisfies group-level consistency while remaining CPD-friendly.

\textbf{Masking.}
Let \(B_t^{(d)}\in\{0,1\}\) indicate whether feature \(d\) of \(u_t\) is originally observed, \(I_t^{(d)}\in\{0,1\}\) whether we \emph{artificially} mask it, and \(M_t^{(d)}=B_t^{(d)}(1-I_t^{(d)})\) be entries visible to the model. The model outputs reconstructed features \(\tilde{u}_t\) and PTS \(s_t\).

\textbf{Feature reconstruction (FR).}
\begin{equation}
\label{eq:fr}
\mathcal{L}_{\mathrm{FR}}
=\frac{\sum_{t,d}M_t^{(d)}\,\ell_d\!\big(\tilde{u}_t^{(d)},u_t^{(d)}\big)}{\sum_{t,d}M_t^{(d)}},
\end{equation}
with \(\ell_d\) MAE for numeric and CE for categorical dimensions.

\textbf{Masked imputation (MI).}
\begin{equation}
\label{eq:mi}
\mathcal{L}_{\mathrm{MI}}
=\frac{\sum_{t,d}I_t^{(d)}\,\ell_d\!\big(\tilde{u}_t^{(d)},u_t^{(d)}\big)}{\sum_{t,d}I_t^{(d)}}.
\end{equation}

\textbf{Aggregate consistency (AC).}
We penalize discrepancies between group-mean PTS and standardized anchors using Huber loss \(\rho_\delta\):
\begin{equation}
\label{eq:ac}
\mathcal{L}_{\mathrm{AC}}
=\frac{1}{|T|}\sum_{k=1}^K |\mathcal{G}_k|\;
\rho_\delta\!\Big(\mu_k(s)-\tilde{r}_k\Big).
\end{equation}
In mini-batch training, replace \(\mathcal{G}_k\) by \(\mathcal{H}_k=\mathcal{G}_k\cap\mathcal{B}\) and weight by \(|\mathcal{H}_k|\), which preserves the per-instruction normalization.

\textbf{Contiguity and normalization.}
We encourage a CPD-friendly contiguous latent with total variation and moment penalties:
\begin{align}
\label{eq:tv}
\mathcal{L}_{\mathrm{TV}}&=\frac{1}{|T|-1}\sum_{t=2}^{|T|} |s_t-s_{t-1}|,\\
\label{eq:norm}
\mathcal{L}_{\mathrm{NORM}}&=\big(\bar{s}\big)^2+\big(\mathrm{Var}(s)-1\big)^2,\quad
\bar{s}=\tfrac{1}{|T|}\sum_{t=1}^{|T|} s_t.
\end{align}

\textbf{Overall objective.}
\begin{equation}
\label{eq:total}
\mathcal{L}=\mathcal{L}_{\mathrm{FR}}+\lambda\,\mathcal{L}_{\mathrm{MI}}+\gamma\,\mathcal{L}_{\mathrm{AC}}+\eta\,\mathcal{L}_{\mathrm{TV}}+\xi\,\mathcal{L}_{\mathrm{NORM}},
\end{equation}
with \(\lambda{=}1\), \(\gamma{=}1\), and \((\eta,\xi)\) tuned on held‑out traces. We instantiate \(f_\theta\) with compact attention-based imputers; any backbone that yields a contiguous latent works.

\textbf{Optional scale calibration.}
If desired, estimate a single scalar \(c\) after training by
\[
c^\star=\frac{\sum_k |\mathcal{G}_k|\,\tilde{r}_k\,\mu_k(s)}{\sum_k |\mathcal{G}_k|\,\tilde{r}_k^2},
\]
and keep it fixed at inference.

\textbf{Affine identifiability (do we really need this?)}
With standardized anchors \(\{\tilde{r}_k\}\) and \(\mathcal{L}_{\mathrm{NORM}}\) enforcing \(\bar{s}\!\approx\!0\), \(\mathrm{Var}(s)\!\approx\!1\), the minimizer of \(\mathcal{L}_{\mathrm{AC}}\) is invariant only to the identity affine map: any \(s'_t=a s_t+b\) that (approximately) minimizes \(\mathcal{L}_{\mathrm{AC}}\) must satisfy \(a\!\approx\!1\), \(b\!\approx\!0\) (under strictly convex loss near 0 and \(\mathrm{Cov}(\mu(s),\tilde{r})>0\)). \emph{Sketch.} Group-averaging yields \(\mu_k(s')=a\mu_k(s)+b\). Minimizing \(\sum_k |\mathcal{G}_k|\rho\big(\mu_k(s')-\tilde{r}_k\big)\) over \((a,b)\) with the stated moment constraints gives normal equations whose unique solution is \(a{=}1\), \(b{=}0\). 


\subsection{PTS Inference at Scale}
Training uses overlapping chunks to control memory; inference concatenates chunkwise PTS with overlap blending. We start with a higher \(\eta\) in \eqref{eq:tv} (to avoid spurious jaggedness) and anneal \(\eta\) so that PTS tightens around genuine transitions near convergence.

\subsection{Change-Point Detection (CPD) on PTS}
\label{subsec:cpd}
We segment \(s_{1:|T|}\) with PELT (Pruned Exact Linear Time) using an \emph{autoregressive} segment cost (AR order \(p\)) from \texttt{ruptures}. We use a single class of \emph{BIC-style} penalties and avoid manual tuning by sweeping a small grid of multipliers:
\[
\beta_j \;=\; \alpha_j\,(p{+}1)\,\log n, \qquad \alpha_j \in \mathcal{A},
\]
where \(n\) is the length of the PTS tile and \(\mathcal{A}\) is a short list (e.g., 8–16 values). For each \(\beta_j\), we run PELT once to obtain change points \(\mathcal{C}_j\). After windowing (next subsection), we select the segmentation whose preserved-instruction count \(P_j\) is closest to the target budget \(D=\lfloor|T|(1-r)\rfloor\) (ties broken by smaller \(P_j\) and fewer segments). This "range over BIC‑style penalties" eliminates manual tuning while retaining PELT’s exactness per \(\beta_j\). For very long traces, we apply CPD in overlapping tiles and deduplicate boundaries near tile edges.

\subsection{Adaptive Instruction Interval Calculation}
\label{instr_removal}
Given the chosen change points \(\mathcal{C}\), we preserve asymmetric windows around them and prune the rest to match a target reduction \(r\). We adopt the following routine.

\begin{algorithm}[!htbp]
\small
\setlength{\textfloatsep}{4pt}
\caption{Adaptive Instruction Interval Calculation with Asymmetric Adjustment}
\label{alg:adaptive_window}
\begin{algorithmic}[1]
\Require Change points \(\mathcal{C}\subseteq\{1,\ldots,|T|\}\), total instructions \(|T|\), target reduction rate \(r\in(0,1)\), tolerance \(\epsilon\in(0,1)\)
\Ensure Preserved instruction intervals \(\mathcal{M}\)
\State \(D \gets \lfloor |T| \times (1 - r) \rfloor\) \Comment{Desired number of instructions}
\State \(W_{\min} \gets 1\); \(W_{\max} \gets |T|\); \(\mathcal{M}^\star \gets \varnothing\); \(P^\star \gets 0\)
\Function{MakeWindow}{$c,W$}
  \State \(\Delta_L \gets \left\lfloor \frac{W-1}{2} \right\rfloor\), \(\Delta_R \gets W-1-\Delta_L\)
  \State \(s \gets c - \Delta_L\), \(e \gets c + \Delta_R\)
  \If{\(s < 1\)} \Comment{Shift right if left boundary exceeded}
    \State \(e \gets \min(|T|, e + (1 - s))\); \(s \gets 1\)
  \EndIf
  \If{\(e > |T|\)} \Comment{Shift left if right boundary exceeded}
    \State \(s \gets \max(1, s - (e - |T|))\); \(e \gets |T|\)
  \EndIf
  \State \Return \([s,e]\)
\EndFunction

\While{\(W_{\min} \leq W_{\max}\)}
    \State \(W \gets \left\lfloor \frac{W_{\min} + W_{\max}}{2} \right\rfloor\)
    \State Initialize empty list \(\mathcal{R}\)
    \For{each \(c \in \mathcal{C}\)} 
        \State Add interval  {MakeWindow}$(c,W)$ to \(\mathcal{R}\)
    \EndFor
    \State \(\mathcal{M} \gets\) MergeOverlaps\((\mathcal{R})\)
    \State \(P \gets \sum_{[s,e] \in \mathcal{M}} (e - s + 1)\)
    \If{\(|P - D| < |P^\star - D|\)} \(\mathcal{M}^\star \gets \mathcal{M}\); \(P^\star \gets P\) \EndIf
    \If{\(|P - D| \leq D \times \epsilon\)} \Return \(\mathcal{M}\) \EndIf
    \If{\(P < D\)} \State \(W_{\min} \gets W + 1\) \Else \State \(W_{\max} \gets W - 1\) \EndIf
\EndWhile
\State \Return \(\mathcal{M}^\star\) \Comment{Best-effort if tolerance unmet}
\end{algorithmic}
\end{algorithm}

In practice, asymmetricity arises near the boundaries or when change points cluster; the MergeOverlaps step ensures contiguous kept regions without duplication. If desired, one can make \(W\) depend on local PTS jump magnitude or model uncertainty, but we use a global \(W\) for determinism and simplicity.

\subsection{A Learning-Free Surrogate (Beta Metric)}
\label{subsec:beta}
When learning is unavailable, we use a heuristic surrogate that is still contiguous and phase-sensitive:
\begin{equation}
\label{eq:beta}
B_t \;=\; \alpha_1\,\ln\big(1+\mathrm{RD}_t\big)\;+\;\alpha_2\,\big|\Delta\mathrm{PC}_t\big|\;+\;\alpha_3\,\big|\Delta r_{g(t)}\big|,
\end{equation}
where \(\mathrm{RD}_t\) is reuse distance, \(\Delta\mathrm{PC}_t\) encodes control-flow change, and \(\Delta r_k=r_k-r_{k-1}\) is the adjacent \emph{group}-anchor gradient, broadcast to instructions in group \(k\). We use \((\alpha_1,\alpha_2,\alpha_3)=(0.5,0.3,0.2)\). CPD and Algorithm~\ref{alg:adaptive_window} apply unchanged to \(B_t\).


\subsection{Examples of PTS Estimation and Beta Metric}
\label{sec3.2}
Figure~\ref{fig:2} (a) compares a naive stepwise anchor signal (forward-filled group IPC) against the Transformer-learned PTS over a 60K-instruction span. The learned PTS smooths within groups while remaining responsive across groups, producing clearer change points and more faithful preserved chunks. Figure~\ref{fig:2}(b) zooms to a 1K-instruction window, where anchor signal (forward-filled IPC) exhibit flat plateaus; the Beta Metric reveals subtle transitions that anchor signal hides. PTS/Beta Metric is a surrogate phase-sensitivity signal anchored to IPC or other measurable aggregates.
\begin{figure}[!htbp]
    \centering
    \captionsetup{skip=3pt}
    \captionsetup[subfigure]{font=footnotesize, skip=1pt}
    \begin{subfigure}[b]{0.99\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{images/intuition.pdf}
        \caption{Transformer-learned PTS vs. forward-filled IPC.}
        \label{fig:ipc_imputation}
    \end{subfigure}
    \begin{subfigure}[b]{0.99\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ipc_change_point_detection.pdf}
        \caption{Heuristic Beta Metric as a surrogate signal.}
        \label{fig:beta_approach}
    \end{subfigure}
    \caption{(a) Case study on a 60K-instruction window (\emph{BFS} trace). Change points and preserved intervals are marked using Transformer-learned PTS. (b) Case study on a 1K-instruction window (\emph{gcc-13B} trace). Top: forward-filled anchors (flat within groups); Bottom: heuristic Beta metric (contiguous, phase-sensitive). Shaded regions denote preserved chunks.}
    \label{fig:2}
\end{figure} 
\subsection{Sanity Bounds for Pruning Error}
Suppose CPD partitions \(T\) into segments \(\{\mathcal{S}_j\}\) within which a length-normalized additive performance functional \(F(t)\) varies by at most \(\Delta_j\). If all true regime boundaries lie inside preserved windows and we remove \(N_{\mathrm{pruned}}\) instructions across segments, then
\[
\big|\widehat{F}-F\big|\ \le\ \frac{1}{|T|}\sum_j \Delta_j\, |\mathcal{S}_j\setminus\text{kept}|\ \le\ \Delta_{\max}\frac{N_{\mathrm{pruned}}}{|T|}
,\quad \Delta_{\max}=\max_j\Delta_j.
\]
Thus, for piecewise-stable behaviors the error scales with the pruned fraction, while preserving transition neighborhoods controls bias.

\subsection{Implementation Notes}
\label{subsec:impl}
\textbf{Backbone and training.} We use attention-based imputation models; the detailed architecture of the model is orthogonal to VitaBeta as long as \eqref{eq:ac}–\eqref{eq:total} are enforced.

\textbf{Tiling.} PTS is produced in overlapping tiles (50\% overlap) and blended linearly. \textbf{CPD.} We use PELT with AR cost (order \(p\in\{2,5,8\}\)) and a \emph{BIC-style} penalty \(\beta=\alpha (p{+}1)\log n\). We sweep a short grid of \(\alpha\) values per tile and select the segmentation whose post-window preserved length best matches the target---no separate penalty schemes are used. 

\textbf{Integration.} Within SimPoint/LoopPoint regions, PTS/CPD runs \emph{inside} each region; preserved chunks are concatenated per region and upstream weights remain unchanged.

```
